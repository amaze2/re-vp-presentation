{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Thirteen* Digital Ways of Looking at Re- in Victorian Poetry\n",
        "date: 11 November 2023\n",
        "author: Adam Mazel, Digital Publishing Librarian\n",
        "institute: Scholarly Communication Department, IUB Libraries\n",
        "---"
      ],
      "id": "cdc456ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rationale / Significance\n",
        "\n",
        "- Analyzing \"Re-\" is apt for text mining\n",
        "    - small details: hard to notice with one's eyes, but easy to notice with computer \"vision\"\n",
        "    - play with focus of analytic lens\n",
        "        - (too-)close reading: microscopic aspects of language\n",
        "        - distant reading: macroscopic methods of analysis\n",
        "\n",
        "## Method\n",
        "\n",
        "- Exploratory Data Analysis (EDA)\n",
        "    - early research: explore data to discover trends and generate hypotheses / basic insights\n",
        "- (simple) Count-based methods, rather than (complex) machine-  / deep-learning methods\n",
        "    - Apt for EDA / early research, feature (\"re-\") analysis, smaller datasets\n",
        "- Python (Natural Language ToolKit (NLTK), Matplotlib)\n",
        "\n",
        "## Data: Which Authors / Texts + Why?\n",
        "\n",
        "- DG Rossetti\n",
        "    - *Poems* (1870)\n",
        "    - *Poems: A New Edition* (1881)\n",
        "    - *Ballads and Sonnets* (1881)\n",
        "- AC Swinburne\n",
        "    - *Atalanta in Calydon* (1865)\n",
        "    - *Poems and Ballads* (1866)\n",
        "    - *Songs Before Sunrise* (1871)\n",
        "    - *Songs of Two Nations* (1875)\n",
        "    - *Erechtheus* (1876)\n",
        "    - *Poems and Ballads, Second Series* (1878)\n",
        "    - *Songs of the Springtides* (1880)\n",
        "    - *Studies in Song* (1880)\n",
        "    - *The Heptalogia, or the Seven against Sense. A Cap with Seven Bells* (1880)\n",
        "    - *Tristram of Lyonesse* (1882)\n",
        "    - *A Century of Roundels* (1883)\n",
        "    - *A Midsummer Holiday and Other Poems* (1884)\n",
        "    - *Poems and Ballads, Third Series* (1889)\n",
        "    - *Astrophel and Other Poems* (1894)\n",
        "    - *The Tale of Balen* (1896)\n",
        "    - *A Channel Passage and Other Poems* (1904)\n",
        "- Michael Field\n",
        "    - *Long Ago* (1889)\n",
        "    - *Sight and Song* (1892)\n",
        "    - *Underneath the Bough* (1893)\n",
        "    - *Wild Honey from Various Thyme* (1908)\n",
        "    - *Poems of Adoration* (1912)\n",
        "    - *Mystic Trees* (1913)\n",
        "    - *Whym Chow: Flame of Love* (1914)\n",
        "- Thomas Hardy\n",
        "    - *Wessex Poems and Other Verses* (1898)\n",
        "    - *Poems of the Past and the Present* (1901)\n",
        "    - *Time's Laughingstocks and Other Verses* (1909)\n",
        "    - *Satires of Circumstance* (1914)\n",
        "    - *Moments of Vision* (1917)\n",
        "    - *Late Lyrics and Earlier with Many Other Verses* (1922)\n",
        "    - *Human Shows, Far Phantasies, Songs and Trifles* (1925)\n",
        "\n",
        "- Where Acquired\n",
        "    - [Project Gutenberg](https://www.gutenberg.org/)\n",
        "    - [HathiTrust](https://www.hathitrust.org/)\n",
        "    - [COVE (Collaborative Organization for Virtual Education)](https://editions.covecollective.org/)\n",
        "\n",
        "- Data Cleaning\n",
        "    - Removed noise: Boilerplate, Title Pages, Tables of Contents, Advertisments, Endorsements, Headers + Foorters (most), Unusual Characters\n",
        "- Data Quality \n",
        "    - OCR: Errors\n",
        "    - Noise (headers + footers, etc.)\n",
        "\n",
        "## Who Uses \"Re-\" Words the Most / Least?: Theory\n",
        "\n",
        "#### How\n",
        "\n",
        "- Comparative Keyword Frequencies\n",
        "    - Count re- words in each poet's corpus \n",
        "    - Normalize counts (percentage of whole) to enable comparison\n",
        "        - divide # of re-words by # of words of each poet's corpus\n",
        "    - Visualize each poet's percentage in bar chart\n",
        "\n",
        "#### Why\n",
        "\n",
        "- Who is (not) interested in \"re-\" words?\n",
        "\n",
        "## Who Uses \"Re-\" Words the Most / Least?: Code"
      ],
      "id": "15e2e6d4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nltk\n",
        "import os\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt')  # Download NLTK tokenizer data\n",
        "\n",
        "\n",
        "# Define a function to remove all punctuation except hyphens\n",
        "def remove_punctuation_except_hyphens(text):\n",
        "    translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Define a function to count \"re-\" words in a given text\n",
        "def count_re_words(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n",
        "\n",
        "# Specify the directory paths for the two poets' corpora\n",
        "corpus_directories = {\n",
        "    'swinburne': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n",
        "    'hardy': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n",
        "    'michael field': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n",
        "    'dg rossetti': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n",
        "    # Add more directories here\n",
        "}\n",
        "\n",
        "# Initialize dictionaries to store the results\n",
        "percentage_re_words = {}\n",
        "\n",
        "# Read, tokenize, and calculate for each poet's corpus\n",
        "for poet, corpus_directory in corpus_directories.items():\n",
        "    corpus = []\n",
        "    \n",
        "    # Read and tokenize the text files in the poet's corpus\n",
        "    for filename in os.listdir(corpus_directory):\n",
        "        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            text = remove_punctuation_except_hyphens(text)\n",
        "            corpus.append(text)\n",
        "\n",
        "    # Count the \"re-\" words in the poet's corpus\n",
        "    re_word_count = sum(count_re_words(text) for text in corpus)\n",
        "\n",
        "    # Calculate the percentage of \"re-\" words in the poet's corpus\n",
        "    total_words = sum(len(nltk.word_tokenize(text)) for text in corpus)\n",
        "    percentage_re_words[poet] = (re_word_count / total_words) * 100\n",
        "\n",
        "# Sort the results from largest to smallest\n",
        "sorted_results = sorted(percentage_re_words.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Extract poets and percentages for plotting\n",
        "poets, percentages = zip(*sorted_results)\n",
        "\n",
        "# Step 4: Create a bar chart to visualize the results with dynamic y-axis limit and sorted labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(poets, percentages, color=['blue', 'orange'])\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.title('Whose Poetry is More Composed of Words that Start with \"Re-\"?')\n",
        "\n",
        "# Set the y-axis limit based on the largest percentage\n",
        "ylim_percentage = max(percentages) * 2  # Adjusted for better visualization\n",
        "plt.ylim(0, ylim_percentage)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Display the bar chart\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "c313ab68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results: Take Aways\n",
        "\n",
        "- Hardy: Uses \"Re-\" Words more\n",
        "- Rossetti: Uses \"Re-\" Words less\n",
        "\n",
        "\n",
        "## Which \"Re-\" Words Are Most Frequent: Theory\n",
        "\n",
        "#### How\n",
        "\n",
        "- Keyword Frequency\n",
        "    - ID which re- words are used and count how often\n",
        "\n",
        "#### Why\n",
        "\n",
        "- Uncover meaningful patterns in language use\n",
        "    - Significant terms\n",
        "    - Style\n",
        "    - Aboutness: Themes / Topics\n",
        "\n",
        "## Which \"Re-\" Words Are Most Frequent: Code\n",
        "\n",
        "- Have computer count all words that start with \"re-\" in poet's corpus + visualize results in bar chart\n",
        "- Stemming\n",
        "    - Remove inflected endings to condense different versions of same term to a common stem\n",
        "    - \"re-enter: 1\", \"re-entering: 1\", \"re-entered: 1\" --> \"re-ent: 3\"\n",
        "    - Improves IDing significant concepts\n"
      ],
      "id": "969499c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import software libraries / dependencies\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('punkt')  # Download NLTK tokenizer data\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")  # Initialize SnowballStemmer for English\n",
        "\n",
        "# create function to preprocess and process files of each directory\n",
        "def process_directory(corpus_directory):   \n",
        "    corpus = []\n",
        "\n",
        "    # Get the directory name as the label\n",
        "    label = os.path.basename(corpus_directory)\n",
        "\n",
        "    # Step 1: Get the text of corpus from files\n",
        "    for filename in os.listdir(corpus_directory):\n",
        "        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            corpus.append(text)\n",
        "\n",
        "    # Step 2: Tokenize the text blob into individual words\n",
        "    tokenized_corpus = [nltk.word_tokenize(text) for text in corpus]\n",
        "\n",
        "    # Step 3: Standardize (lower) case, find words that start with re-, stem those words, retain them\n",
        "    stemmed_corpus = []\n",
        "    for tokens in tokenized_corpus:\n",
        "        stemmed_tokens = [stemmer.stem(word.lower()) for word in tokens if re.match(r'\\b(re-)\\w+', word.lower())]\n",
        "        stemmed_corpus.append(stemmed_tokens)\n",
        "\n",
        "    # Step 6: Count the frequency of each re- word\n",
        "    word_counts = Counter(word for tokens in stemmed_corpus for word in tokens)\n",
        "\n",
        "    # Step 7: Display the most frequent words\n",
        "    # most_common_re_words = word_counts.most_common(20)  # Set the desired number of top words\n",
        "    # for word, count in most_common_re_words:\n",
        "    #    print(f'The poetry of {label[:-5]} uses {word}: {count} times')\n",
        "\n",
        "    # Step 8: Plot words and counts in a bar chart\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    top_words, top_counts = zip(*word_counts.most_common(20))\n",
        "    plt.bar(top_words, top_counts)\n",
        "    plt.title(f'Frequency of Stemmed Re- Words in {label[:-5].capitalize()}\\'s Poetry')\n",
        "    plt.xticks(rotation=65)\n",
        "    plt.show()\n",
        "\n",
        "# Directories to process\n",
        "corpus_directories = [\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n",
        "    # Add more directories here\n",
        "]\n",
        "\n",
        "# Process each directory\n",
        "for directory in corpus_directories:\n",
        "    process_directory(directory)"
      ],
      "id": "a14efae6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results: Take Aways\n",
        "\n",
        "- Keywords\n",
        "    - Swinburne: \"Re-Risen: 12\" (80% of S's \"Re-\" Words)\n",
        "        - \"re-risen\": content: interested in ressurection?\n",
        "        - \"re-risen: form: alliterative (more echoic than \"ressurect\")\n",
        "    - Hardy: \"Re-Enact: 4\" (20% of H's \"Re-\" Words)\n",
        "- Comparison of \"Re-\" Use\n",
        "\n",
        "|                    | Low Vocabulary | High Vocabulary |\n",
        "|------------------- | ---------------| ----------------|\n",
        "| **High Frequency** | Swinburne      |                 |\n",
        "| **Low Frequency**  | Rossetti       | Hardy, Field    |\n",
        "\n",
        "- Rossetti's \"Re-Born\"\n",
        "    - Hapax Legomenon\n",
        "        - [\"Ardour & Memory\"](http://www.rossettiarchive.org/docs/4-1873.tinkerms.rad.html) (1879), Sonnet LXIV, *House of Life* __Ballads and Sonnets__ (1881)\n",
        "            - \"The furtive flickering streams to light re-born / 'Mid airs new-fledged & valorous lusts of morn,\"\n",
        "        - But hyphenated prefixes (e.g. \"a-heap\", \"to-night\") and compound words (e.g. \"cukoo-throb\", \"forest-boughs\") are common (1.22%) in DGR\n",
        "        - \"ressurect*\" appears only once, \"born: 48\" and \"birth: 46\" \n",
        "        - Hypotheses: uninterested in signifying repetition through \"re-\", more interested in birth than rebirth?\n",
        "\n",
        "- Hardy  and Field: \"Re-Illume\" in Context\n",
        "    - Extremely rare (OED Band 1)\n",
        "    - Chiefly poetic\n",
        "    - 1758 - present\n",
        "    - Hardy: 2x\n",
        "        - \"Two Rosalinds\": __Time's Laughingstocks and Other Verses__ (1909), \"For Life I had never cared greatly\": __Moments of Vision and Miscellaneous Verses__ (1917)\n",
        "    - Field: 1x\n",
        "        - [\"XXII: Sleeping together: Sleep\"](https://editions.covecollective.org/edition/whym-chow-flame-love/whym-chow-flame-love#chapter22): __Whym Chow: Flame of Love__ (1906, 1914)\n",
        "\n",
        "## {background-iframe=\"https://books.google.com/ngrams/graph?content=reillumed%2Breillume%2Breilluming%2Breillumes&year_start=1779&year_end=2019&corpus=en-GB-2019&smoothing=3\" background-interactive=\"true\"}\n",
        "\n",
        "## Results: Take Aways\n",
        "\n",
        "    - rare word becoming obsolete\n",
        "    - \"re-illume\" a word declining in use\n",
        "\n",
        "## \"Re-\" Words in Context: Theory\n",
        "\n",
        "#### How\n",
        "\n",
        "- Bigrams (n-grams)\n",
        "    - Bigrams: two consecutive words (Trigram: three consecutive words) \n",
        "        - e.g. \"She used the olive oil.\": \"She used\", \"used the\", \"the olive\", \"olive oil\"\n",
        "    - ID / count bigrams of re- words: re- word + consecutive word\n",
        "\n",
        "#### Why\n",
        "\n",
        "- Key Word in Context: Know a word by the company it keeps: facilitaate understadning of key word\n",
        "    - e.g. \"olive\", \"oil\"--> \"olive oil\"\n",
        "- Better understand re- words by learning their immediate contexts and associations via frequently co-occuring terms\n",
        "    - Fundamental method of text mining (TM)\n",
        "\n",
        "## Which Words Adjoin \"Re-\" Words?: Code\n",
        "\n",
        "- Have computer ID all the bigrams of each poet's works and then filter everything but bigrams of words that start with re- \n",
        "- remove stopwords (meaningless function words) to reveal associated concepts\n"
      ],
      "id": "c8fadf7c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "slide"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# import software dependencies / libraries\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import string\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Directories to process, Aliases \n",
        "corpus_directories = {\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n",
        "}\n",
        "\n",
        "# RegEx to match words starting with \"re-\"\n",
        "re_word_pattern = r'\\b(re-\\w+)'\n",
        "\n",
        "# Dictionary to store directory/poet: bigram frequencies \n",
        "bigram_frequencies = {alias: Counter() for alias in corpus_directories.values()}\n",
        "\n",
        "# Define the punctuation to remove (including curly quotation marks)\n",
        "punctuation_to_remove = string.punctuation.replace('-', '') + \"‘’“”\"\n",
        "\n",
        "# Preprocess and Process each directory\n",
        "for corpus_directory, alias in corpus_directories.items():\n",
        "    # Get the text from the text files in the corpus\n",
        "    for filename in os.listdir(corpus_directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "\n",
        "                # Tokenize the text\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "\n",
        "                # Remove punctuation except hyphens and standardize case\n",
        "                translator = str.maketrans('', '', punctuation_to_remove)\n",
        "                preprocessed_text = ' '.join(tokens).translate(translator).lower()\n",
        "\n",
        "                # Remove stopwords\n",
        "                stop_words = set(stopwords.words(\"english\"))\n",
        "                filtered_tokens = [word for word in preprocessed_text.split() if word not in stop_words]\n",
        "\n",
        "                # Find bigrams\n",
        "                bigrams = list(ngrams(filtered_tokens, 2))\n",
        "\n",
        "                # Filter bigrams to include only those starting with \"re-\"\n",
        "                re_bigrams = [bigram for bigram in bigrams if re.match(re_word_pattern, bigram[0])]\n",
        "\n",
        "                # Count the bigrams\n",
        "                bigram_frequency = Counter(re_bigrams)\n",
        "\n",
        "                # Add the bigram frequencies as values of the current directory/poet as keys\n",
        "                bigram_frequencies[alias].update(bigram_frequency)\n",
        "\n",
        "# Print the sorted bigram frequencies per directory/poet\n",
        "for alias, frequencies in bigram_frequencies.items():\n",
        "    print(f\"{alias}:\")\n",
        "    for bigram, frequency in sorted(frequencies.items(), key=lambda x: x[0]):  # Sort alphabetically\n",
        "        print(f\"{bigram}: {frequency}\")\n",
        "    print()"
      ],
      "id": "8db08852",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results: Take Aways\n",
        "\n",
        "- Swinburne\n",
        "    - \"re-__\" : \"life\", \"dead\" (2x), \"dust\", \"mortal\"\n",
        "        - re- associated with life + death, mortality, etc.\n",
        "    - \"re-risen\": \"prison\"\n",
        "        - re-risen as internal rhyme: \"re-\" \"-ri-\" \"pri-\"\n",
        "- Hardy\n",
        "    - \"re-____\": \"time\" (2x), \"semipiternal\" (everlasting), \"killing\", \"death\", \"lives\" \"new\", \"olden\"\n",
        "        - re- associated with time, eternal / mortal, etc.\n",
        "- Field\n",
        "    - \"re-appear\": \"fade\", \"re-light\": \"tarnished\"\n",
        "        - re- used with opposites\n",
        "\n",
        "## \"Re-\" Words in Context: Theory\n",
        "\n",
        "#### How\n",
        "\n",
        "\n",
        "\n",
        "#### Why\n",
        "\n",
        "\n",
        "## \"Re-\" Words in Context: Code\n",
        "\n",
        "\n",
        "## Results: Take Aways\n",
        "\n",
        "- \"re-\" words often juxtaposed with \"re\" words\n",
        "\n",
        "## When Were \"Re-\" Words More / Less Frequent: Theory\n",
        "\n",
        "#### How\n",
        "\n",
        "- Time Series / Term Frequency over Time\n",
        "- ID when (in poet's career) \"Re-\" words were used more / less often\n",
        "\n",
        "### Why\n",
        "\n",
        "- When in poet's career did \"re-\" become more / less frequent\n",
        "    - Why did \"re-\" wax / wane then?\n",
        "- In which book(s) is re- frequent / infrequent? Why?\n",
        "\n",
        "## When Were \"Re-\" Words More / Less Frequent: Code\n",
        "\n",
        "- Computer counts what percent each book of poetry is \"re-\" words (total number of re- words divided by total number of words)\n",
        "    - nomalizing the data enables comparison\n",
        "- Y axis: re- word percentage of book; X axis: publication year of book\n"
      ],
      "id": "287ac3ef"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "slide"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# Import software libraries / dependencies\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')  # Download NLTK tokenizer data\n",
        "\n",
        "# Define a function to count words that start with \"re-\"\n",
        "def count_re_words(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n",
        "\n",
        "# Define a function to remove all punctuation except hyphens\n",
        "def remove_punctuation_except_hyphens(text):\n",
        "    translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Specify the parent directory containing multiple text file directories\n",
        "parent_directory = '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/'\n",
        "\n",
        "# Soecific irectories to process\n",
        "all_directory_names = [\n",
        "    'swinburne/swinburne_noBP',\n",
        "    'hardy/hardy_noBP',\n",
        "    'field/field_NoBP',\n",
        "    'rossetti_dg/rossetti_dg_NoBP',\n",
        "]\n",
        "\n",
        "# Initialize a dictionary to store year-wise percentages\n",
        "year_percentages = {}\n",
        "\n",
        "# Process each directory\n",
        "for directory_name in all_directory_names:\n",
        "    # Construct the full path to the current directory\n",
        "    text_files_directory = os.path.join(parent_directory, directory_name)\n",
        "\n",
        "    if os.path.isdir(text_files_directory):\n",
        "        # Initialize a dictionary to store year-wise percentages for the current directory\n",
        "        directory_percentages = {}\n",
        "\n",
        "        # Initialize a dictionary to store file names for the current directory\n",
        "        file_names = {}\n",
        "\n",
        "        # Process each text file in the current directory\n",
        "        for filename in os.listdir(text_files_directory):\n",
        "            if filename.endswith('.txt'):\n",
        "                # Extract the year from the filename using a regular expression\n",
        "                year_match = re.match(r'(\\d{4})_', filename)\n",
        "                if year_match:\n",
        "                    year = int(year_match.group(1))\n",
        "                    with open(os.path.join(text_files_directory, filename), 'r', encoding='utf-8') as file:\n",
        "                        text = file.read()\n",
        "                        # Remove punctuation except hyphens\n",
        "                        text = remove_punctuation_except_hyphens(text)\n",
        "                        # normalize counts\n",
        "                        total_words = len(nltk.word_tokenize(text))\n",
        "                        re_word_count = count_re_words(text)\n",
        "                        percentage_re_words = (re_word_count / total_words) * 100\n",
        "                        \n",
        "                        #add to dictionary: values: counts to keys: years\n",
        "                        directory_percentages[year] = percentage_re_words\n",
        "                        \n",
        "                        # Extract the text between underscores in the filename\n",
        "                        file_name_parts = filename.split('_')\n",
        "                        if len(file_name_parts) > 2:\n",
        "                            file_name = '_'.join(file_name_parts[1:-1])\n",
        "                        else:\n",
        "                            file_name = file_name_parts[1]\n",
        "                        \n",
        "                        #add to dictionary: values: filenames to keys: years \n",
        "                        file_names[year] = file_name  # Store the extracted file name\n",
        "\n",
        "        # Sort the dictionary by keys (years) for the current directory\n",
        "        sorted_directory_percentages = {year: directory_percentages[year] for year in sorted(directory_percentages)}\n",
        "\n",
        "        # Store the results for the current directory\n",
        "        year_percentages[directory_name] = {\n",
        "            'percentages': sorted_directory_percentages,\n",
        "            'file_names': file_names  # Store file names for this directory\n",
        "        }\n",
        "\n",
        "# Step 2: Plot the keys (years) and values (percentages) in a line graph for each directory\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "for directory_name, data in year_percentages.items():\n",
        "    percentages = data['percentages']\n",
        "    file_names = data['file_names']\n",
        "    years = list(percentages.keys())\n",
        "    percentages = list(percentages.values())\n",
        "    \n",
        "    # Plot the data points\n",
        "    plt.plot(years, percentages, marker='o', linestyle='-', label=directory_name[:-5])\n",
        "    \n",
        "    # Add annotations for each data point if the value is greater than 0\n",
        "    for year, percentage in zip(years, percentages):\n",
        "        if percentage > 0:\n",
        "            file_name = file_names[year]\n",
        "            annotation = f\"{file_name}\"\n",
        "            plt.annotate(annotation, (year, percentage), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
        "\n",
        "plt.ylim(0, 0.0525)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Percentage of Words Starting with \"Re-\"')\n",
        "plt.title('When Were \"Re-\" Words Used in Field\\'s, Hardy\\'s, DG Rossetti\\'s, and Swinburne\\'s Poetry?')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.legend()  # Show legend indicating directory names\n",
        "\n",
        "# Display the line graph\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e31aed64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results: Take Aways\n",
        "\n",
        "- Swinburne\n",
        "    - \"Re-\": of interest in mid career, especially in __A Century of Roundels__ (1883)\n",
        "        - \"Re-\" as theme of repetition apt for Roundel form: repetitive\n",
        "- Field\n",
        "    - \"Re-\": of very strong interest in __Whym Chow: Flame of Love__ (1914)\n",
        "        - \"Re-\": intimates \"again-ness\", return, and memory: apt for an elegy (dog) \n",
        "\n",
        "## Are \"Re-\" Words Used Positively or Negatively? Theory\n",
        "\n",
        "#### How\n",
        "\n",
        "- Sentiment Analysis\n",
        "- determines (part of) a text's emotional tone (positive / negative / neutral)\n",
        "\n",
        "#### Why\n",
        "\n",
        "- Opinion mining / reception\n",
        "- Characters' sentiments / emotional arcs\n",
        "- Sentiment of plot: ID emotional highs / lows\n",
        "- Emotional valences / connotations of keywords (\"Re-\" words)\n",
        "    - Stylistics: Author's tonal / emotional preferences\n",
        "    - Comparison: compare how poets treat emotional associations of \"Re-\" words\n",
        "\n",
        "## Are \"Re-\" Words Used Positively or Negatively? Code\n",
        "\n",
        "- Sentiment Analyzer: VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
        "    - lexicon-based approach\n",
        "        - VADER employs a lexicon that contains thousands of words and their polarity scores, indicating whether the word is positive, negative, or neutral.\n",
        "        - VADER also considers intensifiers (e.g., \"very\" or \"extremely\") that modify the sentiment of adjacent words.\n",
        "        - VADER also considers punctuation, such as exclamation marks and question marks, which can influence sentiment.\n",
        "        - VADER considers capitalization, giving more weight to fully capitalized words (e.g., \"HAPPY\") and less weight to words in all lowercase.\n",
        "    - limitations: sarcasm, irony, or tone\n",
        "- Sentiment Scores of Sentences with \"Re-\" Words\n",
        "    - aggregate sentiment scores of all sentences with re- words\n",
        "- Setiment Scores of each Corpus and\n",
        "    - aggregate sentiment scores of entire corpus\n",
        "    - serve as a norm against which to compare / contrast Sentiment Scores of Sentences with \"Re-\" Words\n"
      ],
      "id": "2178d212"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "slide"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# Import software libraries / dependencies\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to calculate aggregate sentiment for a collection of sentences\n",
        "def calculate_aggregate_sentiment(sentences):\n",
        "    positive_score = 0\n",
        "    negative_score = 0\n",
        "    neutral_score = 0\n",
        "    total_sentences = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentiment = sia.polarity_scores(sentence)\n",
        "        positive_score += sentiment['pos']\n",
        "        negative_score += sentiment['neg']\n",
        "        neutral_score += sentiment['neu']\n",
        "        total_sentences += 1\n",
        "\n",
        "    if total_sentences > 0:\n",
        "        avg_positive_score = positive_score / total_sentences\n",
        "        avg_negative_score = negative_score / total_sentences\n",
        "        avg_neutral_score = neutral_score / total_sentences\n",
        "\n",
        "        if avg_positive_score > avg_negative_score:\n",
        "            overall_sentiment = \"Positive\"\n",
        "        elif avg_positive_score < avg_negative_score:\n",
        "            overall_sentiment = \"Negative\"\n",
        "        else:\n",
        "            overall_sentiment = \"Neutral\"\n",
        "\n",
        "        return {\n",
        "            \"Total Sentences Analyzed\": total_sentences,\n",
        "            \"Average Positive Score\": avg_positive_score,\n",
        "            \"Average Negative Score\": avg_negative_score,\n",
        "            \"Average Neutral Score\": avg_neutral_score,\n",
        "            \"Overall Sentiment\": overall_sentiment,\n",
        "        }\n",
        "\n",
        "# Specify the directories you want to process with aliases\n",
        "corpus_directories = {\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n",
        "    # Add more directories here\n",
        "}\n",
        "\n",
        "# Create lists to store poet names and their corresponding score differences\n",
        "poet_names = []\n",
        "score_differences = []\n",
        "\n",
        "# Process each directory\n",
        "for corpus_directory, alias in corpus_directories.items():\n",
        "    sentences = []\n",
        "\n",
        "    # Read and preprocess the text files in the corpus\n",
        "    for filename in os.listdir(corpus_directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "                sentences += nltk.sent_tokenize(text)\n",
        "\n",
        "    # Calculate aggregate sentiment\n",
        "    results = calculate_aggregate_sentiment(sentences)\n",
        "\n",
        "    # Calculate the difference between positive and negative scores\n",
        "    positive_score = results[\"Average Positive Score\"]\n",
        "    negative_score = results[\"Average Negative Score\"]\n",
        "    score_difference = positive_score - negative_score\n",
        "\n",
        "    poet_names.append(alias)\n",
        "    score_differences.append(score_difference)\n",
        "\n",
        "# Sort the poet names and score differences by score differences in descending order\n",
        "sorted_poet_names, sorted_score_differences = zip(*sorted(zip(poet_names, score_differences), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "# Create a bar chart of the score differences with poet names on the x-axis\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(sorted_poet_names, sorted_score_differences, color='skyblue')\n",
        "plt.xlabel(\"Poet\")\n",
        "plt.ylabel('Difference between Positive and Negative Scores')\n",
        "plt.title('Whose Poetry Is Most Positive?')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "id": "bf4ccc68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "slide"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import os\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to calculate aggregate sentiment for a collection of sentences\n",
        "def calculate_aggregate_sentiment(sentences):\n",
        "    positive_score = 0\n",
        "    negative_score = 0\n",
        "    neutral_score = 0\n",
        "    total_sentences = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentiment = sia.polarity_scores(sentence)\n",
        "        positive_score += sentiment['pos']\n",
        "        negative_score += sentiment['neg']\n",
        "        neutral_score += sentiment['neu']\n",
        "        total_sentences += 1\n",
        "\n",
        "    if total_sentences > 0:\n",
        "        avg_positive_score = positive_score / total_sentences\n",
        "        avg_negative_score = negative_score / total_sentences\n",
        "        avg_neutral_score = neutral_score / total_sentences\n",
        "\n",
        "        if avg_positive_score > avg_negative_score:\n",
        "            overall_sentiment = \"Positive\"\n",
        "        elif avg_positive_score < avg_negative_score:\n",
        "            overall_sentiment = \"Negative\"\n",
        "        else:\n",
        "            overall_sentiment = \"Neutral\"\n",
        "\n",
        "        return {\n",
        "            \"Total Sentences Analyzed\": total_sentences,\n",
        "            \"Average Positive Score\": avg_positive_score,\n",
        "            \"Average Negative Score\": avg_negative_score,\n",
        "            \"Average Neutral Score\": avg_neutral_score,\n",
        "            \"Overall Sentiment\": overall_sentiment,\n",
        "        }\n",
        "    else:\n",
        "        return {\"No sentences with 're-' words found for analysis.\"}\n",
        "\n",
        "# Specify the directories you want to process with aliases\n",
        "corpus_directories = {\n",
        " \n",
        "        '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n",
        "    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n",
        "       # Add more directories here\n",
        "}\n",
        "\n",
        "# Process each directory\n",
        "for corpus_directory, alias in corpus_directories.items():\n",
        "    sentences = []\n",
        "\n",
        "    # Read and preprocess the text files in the corpus\n",
        "    for filename in os.listdir(corpus_directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "                sentences += nltk.sent_tokenize(text)\n",
        "\n",
        "    # Filter sentences with words starting with \"re-\"\n",
        "    sentences_with_re = [sentence for sentence in sentences if any(word.lower().startswith(\"re-\") for word in nltk.word_tokenize(sentence))]\n",
        "\n",
        "    # Calculate aggregate sentiment for the filtered sentences\n",
        "    results = calculate_aggregate_sentiment(sentences_with_re)\n",
        "\n",
        "    # Print results for the current directory\n",
        "    print(f\"{alias}\")\n",
        "    for key, value in results.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print()"
      ],
      "id": "3b794711",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results: Take Aways\n",
        "\n",
        "|                          | Poetry: More Positive | Poetry: More Negative |\n",
        "|--------------------------| ----------------------| ----------------------|\n",
        "| **\"Re-\": More Positive** | Field                 | Rossetti              |\n",
        "| **\"Re-\": More Negative** | Swinburne             | Hardy                 |\n",
        "\n",
        "- Field: Consistently Positive\n",
        "- Hardy: Consistently Negative\n",
        "- Swinburne: \"Re-\" tends to be in negative contexts / have negative connotations\n",
        "    - ressurection / life after death: negative connotations for Swinburne?\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "- Take Aways\n",
        "    - small details (re-) <--> large data / methods / trends\n",
        "    - No conclusions: EDA\n",
        "\n",
        "- Next Steps\n",
        "    - \"re-\" context / patterns in surrounding language\n",
        "        - collocates\n",
        "    - \"re-\" omissions\n",
        "        - Why so rare in DGR?\n",
        "    - Re-familiarize myself with poetry\n",
        "    - Expand data / scope (other poets)\n",
        "\n",
        "- **Thank You!**\n",
        "- Contact Info\n",
        "    - Adam Mazel\n",
        "    - Digital Publishing Librarian\n",
        "    - Indiana University Bloomington\n",
        "    - [amazel@iu.edu](mailto:amazel@iu.edu)"
      ],
      "id": "b8d45af7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
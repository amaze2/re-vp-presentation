{"title":"Thirteen* Digital Ways of Looking at Re- in Victorian Poetry","markdown":{"yaml":{"title":"Thirteen* Digital Ways of Looking at Re- in Victorian Poetry","date":"11 November 2023","author":"Adam Mazel, Digital Publishing Librarian","institute":"Scholarly Communication Department, IUB Libraries"},"headingText":"Rationale / Significance","containsRefs":false,"markdown":"\n\n\n- Analyzing \"Re-\" is apt for text mining\n    - small details: hard to notice with one's eyes, but easy to notice with computer \"vision\"\n    - play with focus of analytic lens\n        - (too-)close reading: microscopic aspects of language\n        - distant reading: macroscopic methods of analysis\n\n## Method\n\n- **Tool:** Python\n- **Approach:** Exploratory Data Analysis (EDA)\n    - early research: explore data to discover trends and generate hypotheses / basic insights\n- **Methods:** count based, rather than machine learning \n    - Apt for feature (\"re-\") analysis, smaller datasets\n\n## Data: Which Authors / Texts + Why?\n\n- DG Rossetti\n    - *Poems* (1870)\n    - *Poems: A New Edition* (1881)\n    - *Ballads and Sonnets* (1881)\n\n## Data: Which Authors / Texts + Why?\n\n- AC Swinburne\n    - *Atalanta in Calydon* (1865)\n    - *Poems and Ballads* (1866)\n    - *Songs Before Sunrise* (1871)\n    - *Songs of Two Nations* (1875)\n    - *Erechtheus* (1876)\n    - *Poems and Ballads, Second Series* (1878)\n    - *Songs of the Springtides* (1880)\n    - *Studies in Song* (1880)\n    - *The Heptalogia, or the Seven against Sense. A Cap with Seven Bells* (1880)\n    - *Tristram of Lyonesse* (1882)\n    - *A Century of Roundels* (1883)\n    - *A Midsummer Holiday and Other Poems* (1884)\n    - *Poems and Ballads, Third Series* (1889)\n    - *Astrophel and Other Poems* (1894)\n    - *The Tale of Balen* (1896)\n    - *A Channel Passage and Other Poems* (1904)\n\n## Data: Which Authors / Texts + Why?\n\n- Michael Field\n    - *Long Ago* (1889)\n    - *Sight and Song* (1892)\n    - *Underneath the Bough* (1893)\n    - *Wild Honey from Various Thyme* (1908)\n    - *Poems of Adoration* (1912)\n    - *Mystic Trees* (1913)\n    - *Whym Chow: Flame of Love* (1914)\n\n## Data: Which Authors / Texts + Why?\n\n- Thomas Hardy\n    - *Wessex Poems and Other Verses* (1898)\n    - *Poems of the Past and the Present* (1901)\n    - *Time's Laughingstocks and Other Verses* (1909)\n    - *Satires of Circumstance* (1914)\n    - *Moments of Vision* (1917)\n    - *Late Lyrics and Earlier with Many Other Verses* (1922)\n    - *Human Shows, Far Phantasies, Songs and Trifles* (1925)\n\n## Data: Which Authors / Texts + Why?\n\n- Where Acquired\n    - [Project Gutenberg](https://www.gutenberg.org/)\n    - [HathiTrust](https://www.hathitrust.org/)\n    - [COVE (Collaborative Organization for Virtual Education)](https://editions.covecollective.org/)\n\n## Data: Cleaning / Quality\n\n- Data Cleaning\n    - Removed noise: Boilerplate, Title Pages, Tables of Contents, Advertisments, Endorsements, Headers + Footers (most), Unusual Characters\n- Data Quality \n    - OCR: Errors\n    - Pub info: headers + footers, introductions / conclusions, dedications, etc.\n\n## Who Uses \"Re-\" Words the Most / Least?\n\n- What percent of each poet’s corpus is composed of words that start with the prefix \"re-\"?\n\n- Whose poetry is most / least composed of “re-” words? \n- Who uses \"re-\" words more / less? \n\n## Who Uses \"Re-\" Words the Most / Least?\n\n- Compare Keyword Percentages\n    - Count \"re-\"\" words in each poet's corpus \n    - Normalize counts to enable comparison\n        - Divide # of \"re-\" words by # of words of each poet's corpus\n    - Visualize each poet's percentage of \"re-\" words in bar chart\n\n```{python}\nimport nltk\nimport os\n# import string\nimport matplotlib.pyplot as plt\n\nnltk.download('punkt')  # Download NLTK tokenizer data\n\n\n# Define a function to remove all punctuation except hyphens\n# def remove_punctuation_except_hyphens(text):\n    # translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n    # return text.translate(translator)\n\n# Define a function to count \"re-\" words in a given text\ndef count_re_words(text):\n    words = nltk.word_tokenize(text)\n    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n\n# Specify the directory paths for the two poets' corpora\ncorpus_directories = {\n    'swinburne': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n    'hardy': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n    'michael field': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n    'dg rossetti': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n    # Add more directories here\n}\n\n# Initialize dictionaries to store the results\npercentage_re_words = {}\n\n# Read, tokenize, and calculate for each poet's corpus\nfor poet, corpus_directory in corpus_directories.items():\n    corpus = []\n    \n    # Read and tokenize the text files in the poet's corpus\n    for filename in os.listdir(corpus_directory):\n        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n            text = file.read()\n            # text = remove_punctuation_except_hyphens(text)\n            corpus.append(text)\n\n    # Count the \"re-\" words in the poet's corpus\n    re_word_count = sum(count_re_words(text) for text in corpus)\n\n    # Calculate the percentage of \"re-\" words in the poet's corpus\n    total_words = sum(len(nltk.word_tokenize(text)) for text in corpus)\n    percentage_re_words[poet] = (re_word_count / total_words) * 100\n\n# Sort the results from largest to smallest\nsorted_results = sorted(percentage_re_words.items(), key=lambda x: x[1], reverse=True)\n\n# Extract poets and percentages for plotting\npoets, percentages = zip(*sorted_results)\n\n# Step 4: Create a bar chart to visualize the results with dynamic y-axis limit and sorted labels\nplt.figure(figsize=(8, 6))\nplt.bar(poets, percentages, color=['blue', 'orange'])\nplt.ylabel('Percentage (%)')\nplt.title('Whose Poetry is More Composed of Words that Start with \"Re-\"?')\n\n# Set the y-axis limit based on the largest percentage\nylim_percentage = max(percentages) * 2  # Adjusted for better visualization\nplt.ylim(0, ylim_percentage)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Display the bar chart\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n## Results: Take Aways\n\n- Hardy: Uses \"Re-\" Words Most\n- Rossetti: Uses \"Re-\" Words Least\n\n## Which \"Re-\" Words Are Most Frequent?\n\n- Which \"re-\" words are used and how often? \n\n- ID trends in \"re-\" use: \n    - Which \"re-\" words are keywords? \n        - Themes \n    - One word many times? Many words one time?\n        - Style\n\n## Which \"Re-\" Words Are Most Frequent: Code\n\n- Keyword Frequency\n    - Have computer count all words that start with \"re-\" in poet's corpus \n    - Stemming\n        - Remove inflected endings to condense different versions of same term to a common stem\n        - \"re-enter: 1\", \"re-entering: 1\", \"re-entered: 1\" --> \"re-ent: 3\"\n        - Improves IDing significant concepts\n    - Visualize results in bar chart\n\n```{python}\n \n# import software libraries / dependencies\nimport nltk\nimport os\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('punkt')  # Download NLTK tokenizer data\n\nstemmer = SnowballStemmer(\"english\")  # Initialize SnowballStemmer for English\n\n# create function to preprocess and process files of each directory\ndef process_directory(corpus_directory):   \n    corpus = []\n\n    # Get the directory name as the label\n    label = os.path.basename(corpus_directory)\n\n    # Step 1: Get the text of corpus from files\n    for filename in os.listdir(corpus_directory):\n        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n            text = file.read()\n            corpus.append(text)\n\n    # Step 2: Tokenize the text blob into individual words\n    tokenized_corpus = [nltk.word_tokenize(text) for text in corpus]\n\n    # Step 3: Standardize (lower) case, find words that start with re-, stem those words, retain them\n    stemmed_corpus = []\n    for tokens in tokenized_corpus:\n        stemmed_tokens = [stemmer.stem(word.lower()) for word in tokens if re.match(r'\\b(re-)\\w+', word.lower())]\n        stemmed_corpus.append(stemmed_tokens)\n\n    # Step 6: Count the frequency of each re- word\n    word_counts = Counter(word for tokens in stemmed_corpus for word in tokens)\n\n    # Step 7: Display the most frequent words\n    # most_common_re_words = word_counts.most_common(20)  # Set the desired number of top words\n    # for word, count in most_common_re_words:\n    #    print(f'The poetry of {label[:-5]} uses {word}: {count} times')\n\n    # Step 8: Plot words and counts in a bar chart\n    plt.figure(figsize=(10, 5))\n    top_words, top_counts = zip(*word_counts.most_common(20))\n    plt.bar(top_words, top_counts)\n    plt.title(f'Frequency of Stemmed Re- Words in {label[:-5].capitalize()}\\'s Poetry')\n    plt.xticks(rotation=65)\n    plt.show()\n\n# Directories to process\ncorpus_directories = [\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n    # Add more directories here\n]\n\n# Process each directory\nfor directory in corpus_directories:\n    process_directory(directory)\n```\n\n## Results: Take Aways\n\n- Comparison of \"Re-\" Use\n\n|                    | Low Vocabulary | High Vocabulary |\n|------------------- | ---------------| ----------------|\n| **High Frequency** | Swinburne      |                 |\n| **Low Frequency**  | Rossetti       | Hardy, Field    |\n\n## Results: Take Aways\n\n- Swinburne: \"Re-Risen\"\n    - 12x (80% of S's \"Re-\" Words)\n        - \"re-risen: form: alliterative \n        - \"re-risen\": content: interested in ressurection?\n\n\n## Results: Take Aways\n\n- DG Rossetti's \"Re-Born\"\n    - Hapax Legomenon\n    - But hyphenated prefixes (e.g. \"a-heap\", \"to-night\") and compound words (e.g. \"cukoo-throb\", \"forest-boughs\") are common (1.22%) in DGR's poetry\n    - \"born\": 48x and \"birth\": 46x \n        - \"ressurect*\", like \"re-born,\" appears only once\n \n    - Hypotheses:\n        - uninterested in signifying repetition through \"re-\"\n        - more interested in birth than rebirth?\n\n## Results: Take Aways\n\n- Hardy and Field: \"Re-Illume\" in Context\n    - Extremely rare\n    - Chiefly poetic\n    - 1758 - present\n    - Hardy: 2x\n        - \"Two Rosalinds\": *Time's Laughingstocks and Other Verses* (1909)\n        - \"For Life I had never cared greatly\": *Moments of Vision and Miscellaneous Verses* (1917)\n    - Field: 1x\n        - [\"XXII: Sleeping together: Sleep\"](https://editions.covecollective.org/edition/whym-chow-flame-love/whym-chow-flame-love#chapter22): *Whym Chow: Flame of Love* (1906, 1914)\n\n## {background-iframe=\"https://books.google.com/ngrams/graph?content=reillumed%2Breillume%2Breilluming%2Breillumes&year_start=1779&year_end=2019&corpus=en-GB-2019&smoothing=3\" background-interactive=\"true\"}\n\n## Which Words Adjoin \"Re-\" Words?\n\n- J. R. Firth (Linguist): \"You shall know a word by the company it keeps\" (1957)\n- \"re-\" words: IDing (frequently) co-occuring terms can better clarify their meaning\n\n## Which Words Adjoin \"Re-\" Words?\n\n- Bigrams\n    - Bigrams: two consecutive words\n        - e.g. \"She used the olive oil.\": \"She used\": 1, \"used the\": 1, \"the olive\": 1, \"olive oil\": 1\n    - Have computer ID / count bigrams of re- words: re- word + consecutive word\n    - remove stopwords (meaningless function words) to better reveal associated concepts\n\n```{python}\n# import software dependencies / libraries\nimport nltk\nimport os\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom collections import Counter\nimport string\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('stopwords')\n\n# Directories to process, Aliases \ncorpus_directories = {\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n}\n\n# RegEx to match words starting with \"re-\"\nre_word_pattern = r'\\b(re-\\w+)'\n\n# Dictionary to store directory/poet: bigram frequencies \nbigram_frequencies = {alias: Counter() for alias in corpus_directories.values()}\n\n# Define the punctuation to remove (including curly quotation marks)\npunctuation_to_remove = string.punctuation.replace('-', '') + \"‘’“”\"\n\n# Preprocess and Process each directory\nfor corpus_directory, alias in corpus_directories.items():\n    # Get the text from the text files in the corpus\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n\n                # Tokenize the text\n                tokens = nltk.word_tokenize(text)\n\n                # Remove punctuation except hyphens and standardize case\n                translator = str.maketrans('', '', punctuation_to_remove)\n                preprocessed_text = ' '.join(tokens).translate(translator).lower()\n\n                # Remove stopwords\n                stop_words = set(stopwords.words(\"english\"))\n                filtered_tokens = [word for word in preprocessed_text.split() if word not in stop_words]\n\n                # Find bigrams\n                bigrams = list(ngrams(filtered_tokens, 2))\n\n                # Filter bigrams to include only those starting with \"re-\"\n                re_bigrams = [bigram for bigram in bigrams if re.match(re_word_pattern, bigram[0])]\n\n                # Count the bigrams\n                bigram_frequency = Counter(re_bigrams)\n\n                # Add the bigram frequencies as values of the current directory/poet as keys\n                bigram_frequencies[alias].update(bigram_frequency)\n\n# Print the sorted bigram frequencies per directory/poet\nfor alias, frequencies in bigram_frequencies.items():\n    print(f\"{alias}:\")\n    for bigram, frequency in sorted(frequencies.items(), key=lambda x: x[0]):  # Sort alphabetically\n        print(f\"{bigram}: {frequency}\")\n    print()\n```\n\n## Results: Take Aways\n\n- Swinburne\n    - \"re-__\" : \"life\", \"dead\" (2x), \"dust\", \"mortal\"\n        - re- associated with life + death, mortality, etc.\n    - \"re-risen\": \"prison\": rhyme\n- Hardy\n    - \"re-____\": \"time\" (2x), \"semipiternal\" (everlasting), \"killing\", \"death\", \"lives\" \"new\", \"olden\"\n        - re- associated with time, eternity / mortality, etc.\n- Field\n    - \"re-appear\": \"fade\", \"re-light\": \"tarnished\"\n        - re- used with opposites\n\n## Do \"re-\" words and \"re\" words co-occur in Hardy's poetry?\n\n- Does Hardy juxtapose \"re-\" words and \"re\" words? \n\n## Do \"re-\" words and \"re\" words co-occur in Hardy's poetry?\n\n- Have Python return Hardy's sentences that contain both a word(s) that start with \"re-\" and a word(s) that start with \"re\"\n\n```{python}\nimport os\nimport nltk\nimport re\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n# Define regular expressions\nre_pattern = r'\\b(?:[Rr]e|[Rr]E)\\w+\\b'  # Matches \"re\" followed by one or more letters\nre_hyphen_pattern = r'\\b(?:[Rr]e-|[Rr]E-)\\w+\\b'  # Matches \"re-\" followed by one or more word characters\n\n# \\b matches a word boundary.\n# (?:[Rr]e|[Rr]E) is a non-capturing group that matches either \"re\" or \"RE,\" regardless of case.\n# \\w+ matches one or more word characters following \"re-\" or \"RE-.\"\n\ndef matches_pattern(word, pattern):\n    return re.search(pattern, word, re.IGNORECASE) is not None\n\n# Function to replace matched words with bolded HTML tags\ndef bold_matched_words(match, pattern):\n    return f'**{match.group()}**'\n\n# Directory containing text files\ntext_directory = \"/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP\"\n\n# Iterate through text files in the directory\nfor filename in os.listdir(text_directory):\n    if filename.endswith(\".txt\"):\n        with open(os.path.join(text_directory, filename), \"r\", encoding=\"utf-8\") as file:\n            text = file.read()\n            sentences = sent_tokenize(text)\n            \n            for sentence in sentences:\n                words = word_tokenize(sentence)\n                has_re = any(matches_pattern(word, re_pattern) for word in words)\n                has_re_hyphen = any(matches_pattern(word, re_hyphen_pattern) for word in words)\n                \n                if has_re and has_re_hyphen:\n                    sentence_with_bold = re.sub(re_pattern, lambda x: bold_matched_words(x, re_pattern), sentence)\n                    sentence_with_bold = re.sub(re_hyphen_pattern, lambda x: bold_matched_words(x, re_hyphen_pattern), sentence_with_bold)\n                    print(sentence_with_bold)\n                    print(\"\\n\" * 2)\n```\n\n## Results: Take Aways\n\n- 30% (6 / 20) of sentences containing \"re-\" words in Hardy's poetry also contain \"re\" words\n- Hardy: \"re-\" words co-occur with \"re\" words\n\n- Hardy's style\n    - echo in form: visual and audial rhyme\n    - echo in content: re(-) returns\n\n## When Were \"Re-\" Words More / Less Frequent\n\n- ID when (in poet's career) \"re-\" words were used more / less often\n    - Why did \"re-\" wax / wane then?\n- In which book(s) is re- frequent / infrequent? Why?\n\n## When Were \"Re-\" Words More / Less Frequent\n\n- Time Series / Term Frequency over Time\n    - Y axis: re- word percentage of book\n    - X axis: publication year of book\n\n```{python}\n# Import software libraries / dependencies\nimport nltk\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport string\n\nnltk.download('punkt')  # Download NLTK tokenizer data\n\n# Define a function to count words that start with \"re-\"\ndef count_re_words(text):\n    words = nltk.word_tokenize(text)\n    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n\n# Define a function to remove all punctuation except hyphens\ndef remove_punctuation_except_hyphens(text):\n    translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n    return text.translate(translator)\n\n# Specify the parent directory containing multiple text file directories\nparent_directory = '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/'\n\n# Soecific irectories to process\nall_directory_names = [\n    'swinburne/swinburne_noBP',\n    'hardy/hardy_noBP',\n    'field/field_NoBP',\n    'rossetti_dg/rossetti_dg_NoBP',\n]\n\n# Initialize a dictionary to store year-wise percentages\nyear_percentages = {}\n\n# Process each directory\nfor directory_name in all_directory_names:\n    # Construct the full path to the current directory\n    text_files_directory = os.path.join(parent_directory, directory_name)\n\n    if os.path.isdir(text_files_directory):\n        # Initialize a dictionary to store year-wise percentages for the current directory\n        directory_percentages = {}\n\n        # Initialize a dictionary to store file names for the current directory\n        file_names = {}\n\n        # Process each text file in the current directory\n        for filename in os.listdir(text_files_directory):\n            if filename.endswith('.txt'):\n                # Extract the year from the filename using a regular expression\n                year_match = re.match(r'(\\d{4})_', filename)\n                if year_match:\n                    year = int(year_match.group(1))\n                    with open(os.path.join(text_files_directory, filename), 'r', encoding='utf-8') as file:\n                        text = file.read()\n                        # Remove punctuation except hyphens\n                        text = remove_punctuation_except_hyphens(text)\n                        # normalize counts\n                        total_words = len(nltk.word_tokenize(text))\n                        re_word_count = count_re_words(text)\n                        percentage_re_words = (re_word_count / total_words) * 100\n                        \n                        #add to dictionary: values: counts to keys: years\n                        directory_percentages[year] = percentage_re_words\n                        \n                        # Extract the text between underscores in the filename\n                        file_name_parts = filename.split('_')\n                        if len(file_name_parts) > 2:\n                            file_name = '_'.join(file_name_parts[1:-1])\n                        else:\n                            file_name = file_name_parts[1]\n                        \n                        #add to dictionary: values: filenames to keys: years \n                        file_names[year] = file_name  # Store the extracted file name\n\n        # Sort the dictionary by keys (years) for the current directory\n        sorted_directory_percentages = {year: directory_percentages[year] for year in sorted(directory_percentages)}\n\n        # Store the results for the current directory\n        year_percentages[directory_name] = {\n            'percentages': sorted_directory_percentages,\n            'file_names': file_names  # Store file names for this directory\n        }\n\n# Step 2: Plot the keys (years) and values (percentages) in a line graph for each directory\nplt.figure(figsize=(15, 8))\n\nfor directory_name, data in year_percentages.items():\n    percentages = data['percentages']\n    file_names = data['file_names']\n    years = list(percentages.keys())\n    percentages = list(percentages.values())\n    \n    # Plot the data points\n    plt.plot(years, percentages, marker='o', linestyle='-', label=directory_name[:-5])\n    \n    # Add annotations for each data point if the value is greater than 0\n    for year, percentage in zip(years, percentages):\n        if percentage > 0:\n            file_name = file_names[year]\n            annotation = f\"{file_name}\"\n            plt.annotate(annotation, (year, percentage), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n\nplt.ylim(0, 0.0525)\nplt.xlabel('Year')\nplt.ylabel('Percentage of Words Starting with \"Re-\"')\nplt.title('When Were \"Re-\" Words Used in Field\\'s, Hardy\\'s, DG Rossetti\\'s, and Swinburne\\'s Poetry?')\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.legend()  # Show legend indicating directory names\n\n# Display the line graph\nplt.tight_layout()\nplt.show()\n```\n\n## Results: Take Aways\n\n- Swinburne\n    - \"Re-\": of interest in mid career, especially in *A Century of Roundels* (1883)\n        - \"Re-\": prefix (repetition) apt for Roundel's repetitive form\n- Field\n    - \"Re-\": of interest in *Whym Chow: Flame of Love* (1914)\n        - \"Re-\": prefix (\"again-ness,\" return, and memory) apt for an elegy \n\n## Are \"Re-\" Words Used Positively or Negatively?\n\n- Do \"re-\" words appear more often in contexts that are positive or negative?\n- Thematics: what do \"re-\" words + repetition connote?\n- Stylistics: poet's emotional associations of \"re-\" words\n\n## Are \"Re-\" Words Used Positively or Negatively?\n\n- Sentiment Analysis\n    - determines (part of) a text's emotional tone (positive / negative / neutral)\n    - aggregate sentiment scores of all sentences with re- words\n    - compare to sentiment score of poet's corpus (norm)\n- Sentiment Analyzer: VADER (Valence Aware Dictionary and sEntiment Reasoner)\n    - lexicon-based approach\n        - considers intensifiers, valence shifting, & punctuation\n        - struggles with sarcasm\n```{python}\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport os\nimport re\nimport matplotlib.pyplot as plt\n\n# Initialize the VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Function to calculate aggregate sentiment for a collection of sentences\ndef calculate_aggregate_sentiment(sentences):\n    positive_score = 0\n    negative_score = 0\n    neutral_score = 0\n    total_sentences = 0\n\n    for sentence in sentences:\n        sentiment = sia.polarity_scores(sentence)\n        positive_score += sentiment['pos']\n        negative_score += sentiment['neg']\n        neutral_score += sentiment['neu']\n        total_sentences += 1\n\n    if total_sentences > 0:\n        avg_positive_score = positive_score / total_sentences\n        avg_negative_score = negative_score / total_sentences\n        avg_neutral_score = neutral_score / total_sentences\n\n        if avg_positive_score > avg_negative_score:\n            overall_sentiment = \"Positive\"\n        elif avg_positive_score < avg_negative_score:\n            overall_sentiment = \"Negative\"\n        else:\n            overall_sentiment = \"Neutral\"\n\n        return {\n            \"Total Sentences Analyzed\": total_sentences,\n            \"Average Positive Score\": avg_positive_score,\n            \"Average Negative Score\": avg_negative_score,\n            \"Average Neutral Score\": avg_neutral_score,\n            \"Overall Sentiment\": overall_sentiment,\n        }\n\n# Specify the directories you want to process with aliases\ncorpus_directories = {\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n    # Add more directories here\n}\n\n# Create lists to store poet names and their corresponding score differences\npoet_names = []\nscore_differences = []\n\n# Process each directory\nfor corpus_directory, alias in corpus_directories.items():\n    sentences = []\n\n    # Read and preprocess the text files in the corpus\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n                sentences += nltk.sent_tokenize(text)\n\n    # Calculate aggregate sentiment\n    results = calculate_aggregate_sentiment(sentences)\n\n    # Calculate the difference between positive and negative scores\n    positive_score = results[\"Average Positive Score\"]\n    negative_score = results[\"Average Negative Score\"]\n    score_difference = positive_score - negative_score\n\n    poet_names.append(alias)\n    score_differences.append(score_difference)\n\n# Sort the poet names and score differences by score differences in descending order\nsorted_poet_names, sorted_score_differences = zip(*sorted(zip(poet_names, score_differences), key=lambda x: x[1], reverse=True))\n\n# Create a bar chart of the score differences with poet names on the x-axis\nplt.figure(figsize=(10, 6))\nplt.bar(sorted_poet_names, sorted_score_differences, color='skyblue')\nplt.xlabel(\"Poet\")\nplt.ylabel('Difference between Positive and Negative Scores')\nplt.title('Whose Poetry Is Most Positive?')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y')\nplt.show()\n```\n\n```{python}\n#| echo: true\n#| output-location: slide\n\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport os\n\n# Initialize the VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Function to calculate aggregate sentiment for a collection of sentences\ndef calculate_aggregate_sentiment(sentences):\n    positive_score = 0\n    negative_score = 0\n    neutral_score = 0\n    total_sentences = 0\n\n    for sentence in sentences:\n        sentiment = sia.polarity_scores(sentence)\n        positive_score += sentiment['pos']\n        negative_score += sentiment['neg']\n        neutral_score += sentiment['neu']\n        total_sentences += 1\n\n    if total_sentences > 0:\n        avg_positive_score = positive_score / total_sentences\n        avg_negative_score = negative_score / total_sentences\n        avg_neutral_score = neutral_score / total_sentences\n\n        if avg_positive_score > avg_negative_score:\n            overall_sentiment = \"Positive\"\n        elif avg_positive_score < avg_negative_score:\n            overall_sentiment = \"Negative\"\n        else:\n            overall_sentiment = \"Neutral\"\n\n        return {\n            \"Total Sentences Analyzed\": total_sentences,\n            \"Average Positive Score\": avg_positive_score,\n            \"Average Negative Score\": avg_negative_score,\n            \"Average Neutral Score\": avg_neutral_score,\n            \"Overall Sentiment\": overall_sentiment,\n        }\n    else:\n        return {\"No sentences with 're-' words found for analysis.\"}\n\n# Specify the directories you want to process with aliases\ncorpus_directories = {\n \n        '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n       # Add more directories here\n}\n\n# Process each directory\nfor corpus_directory, alias in corpus_directories.items():\n    sentences = []\n\n    # Read and preprocess the text files in the corpus\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n                sentences += nltk.sent_tokenize(text)\n\n    # Filter sentences with words starting with \"re-\"\n    sentences_with_re = [sentence for sentence in sentences if any(word.lower().startswith(\"re-\") for word in nltk.word_tokenize(sentence))]\n\n    # Calculate aggregate sentiment for the filtered sentences\n    results = calculate_aggregate_sentiment(sentences_with_re)\n\n    # Print results for the current directory\n    print(f\"{alias}\")\n    for key, value in results.items():\n        print(f\"{key}: {value}\")\n    print()\n```\n\n## Results: Take Aways\n\n|                          | Poetry: More Positive | Poetry: More Negative |\n|--------------------------| ----------------------| ----------------------|\n| **\"Re-\": More Positive** | Field                 | Rossetti              |\n| **\"Re-\": More Negative** | Swinburne             | Hardy                 |\n\n## Wrap Up + Thank You!\n\n- Contact Info\n    - Adam Mazel\n    - Digital Publishing Librarian\n    - Indiana University Bloomington\n    - [amazel@iu.edu](mailto:amazel@iu.edu)","srcMarkdownNoYaml":"\n\n## Rationale / Significance\n\n- Analyzing \"Re-\" is apt for text mining\n    - small details: hard to notice with one's eyes, but easy to notice with computer \"vision\"\n    - play with focus of analytic lens\n        - (too-)close reading: microscopic aspects of language\n        - distant reading: macroscopic methods of analysis\n\n## Method\n\n- **Tool:** Python\n- **Approach:** Exploratory Data Analysis (EDA)\n    - early research: explore data to discover trends and generate hypotheses / basic insights\n- **Methods:** count based, rather than machine learning \n    - Apt for feature (\"re-\") analysis, smaller datasets\n\n## Data: Which Authors / Texts + Why?\n\n- DG Rossetti\n    - *Poems* (1870)\n    - *Poems: A New Edition* (1881)\n    - *Ballads and Sonnets* (1881)\n\n## Data: Which Authors / Texts + Why?\n\n- AC Swinburne\n    - *Atalanta in Calydon* (1865)\n    - *Poems and Ballads* (1866)\n    - *Songs Before Sunrise* (1871)\n    - *Songs of Two Nations* (1875)\n    - *Erechtheus* (1876)\n    - *Poems and Ballads, Second Series* (1878)\n    - *Songs of the Springtides* (1880)\n    - *Studies in Song* (1880)\n    - *The Heptalogia, or the Seven against Sense. A Cap with Seven Bells* (1880)\n    - *Tristram of Lyonesse* (1882)\n    - *A Century of Roundels* (1883)\n    - *A Midsummer Holiday and Other Poems* (1884)\n    - *Poems and Ballads, Third Series* (1889)\n    - *Astrophel and Other Poems* (1894)\n    - *The Tale of Balen* (1896)\n    - *A Channel Passage and Other Poems* (1904)\n\n## Data: Which Authors / Texts + Why?\n\n- Michael Field\n    - *Long Ago* (1889)\n    - *Sight and Song* (1892)\n    - *Underneath the Bough* (1893)\n    - *Wild Honey from Various Thyme* (1908)\n    - *Poems of Adoration* (1912)\n    - *Mystic Trees* (1913)\n    - *Whym Chow: Flame of Love* (1914)\n\n## Data: Which Authors / Texts + Why?\n\n- Thomas Hardy\n    - *Wessex Poems and Other Verses* (1898)\n    - *Poems of the Past and the Present* (1901)\n    - *Time's Laughingstocks and Other Verses* (1909)\n    - *Satires of Circumstance* (1914)\n    - *Moments of Vision* (1917)\n    - *Late Lyrics and Earlier with Many Other Verses* (1922)\n    - *Human Shows, Far Phantasies, Songs and Trifles* (1925)\n\n## Data: Which Authors / Texts + Why?\n\n- Where Acquired\n    - [Project Gutenberg](https://www.gutenberg.org/)\n    - [HathiTrust](https://www.hathitrust.org/)\n    - [COVE (Collaborative Organization for Virtual Education)](https://editions.covecollective.org/)\n\n## Data: Cleaning / Quality\n\n- Data Cleaning\n    - Removed noise: Boilerplate, Title Pages, Tables of Contents, Advertisments, Endorsements, Headers + Footers (most), Unusual Characters\n- Data Quality \n    - OCR: Errors\n    - Pub info: headers + footers, introductions / conclusions, dedications, etc.\n\n## Who Uses \"Re-\" Words the Most / Least?\n\n- What percent of each poet’s corpus is composed of words that start with the prefix \"re-\"?\n\n- Whose poetry is most / least composed of “re-” words? \n- Who uses \"re-\" words more / less? \n\n## Who Uses \"Re-\" Words the Most / Least?\n\n- Compare Keyword Percentages\n    - Count \"re-\"\" words in each poet's corpus \n    - Normalize counts to enable comparison\n        - Divide # of \"re-\" words by # of words of each poet's corpus\n    - Visualize each poet's percentage of \"re-\" words in bar chart\n\n```{python}\nimport nltk\nimport os\n# import string\nimport matplotlib.pyplot as plt\n\nnltk.download('punkt')  # Download NLTK tokenizer data\n\n\n# Define a function to remove all punctuation except hyphens\n# def remove_punctuation_except_hyphens(text):\n    # translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n    # return text.translate(translator)\n\n# Define a function to count \"re-\" words in a given text\ndef count_re_words(text):\n    words = nltk.word_tokenize(text)\n    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n\n# Specify the directory paths for the two poets' corpora\ncorpus_directories = {\n    'swinburne': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n    'hardy': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n    'michael field': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n    'dg rossetti': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n    # Add more directories here\n}\n\n# Initialize dictionaries to store the results\npercentage_re_words = {}\n\n# Read, tokenize, and calculate for each poet's corpus\nfor poet, corpus_directory in corpus_directories.items():\n    corpus = []\n    \n    # Read and tokenize the text files in the poet's corpus\n    for filename in os.listdir(corpus_directory):\n        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n            text = file.read()\n            # text = remove_punctuation_except_hyphens(text)\n            corpus.append(text)\n\n    # Count the \"re-\" words in the poet's corpus\n    re_word_count = sum(count_re_words(text) for text in corpus)\n\n    # Calculate the percentage of \"re-\" words in the poet's corpus\n    total_words = sum(len(nltk.word_tokenize(text)) for text in corpus)\n    percentage_re_words[poet] = (re_word_count / total_words) * 100\n\n# Sort the results from largest to smallest\nsorted_results = sorted(percentage_re_words.items(), key=lambda x: x[1], reverse=True)\n\n# Extract poets and percentages for plotting\npoets, percentages = zip(*sorted_results)\n\n# Step 4: Create a bar chart to visualize the results with dynamic y-axis limit and sorted labels\nplt.figure(figsize=(8, 6))\nplt.bar(poets, percentages, color=['blue', 'orange'])\nplt.ylabel('Percentage (%)')\nplt.title('Whose Poetry is More Composed of Words that Start with \"Re-\"?')\n\n# Set the y-axis limit based on the largest percentage\nylim_percentage = max(percentages) * 2  # Adjusted for better visualization\nplt.ylim(0, ylim_percentage)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Display the bar chart\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n## Results: Take Aways\n\n- Hardy: Uses \"Re-\" Words Most\n- Rossetti: Uses \"Re-\" Words Least\n\n## Which \"Re-\" Words Are Most Frequent?\n\n- Which \"re-\" words are used and how often? \n\n- ID trends in \"re-\" use: \n    - Which \"re-\" words are keywords? \n        - Themes \n    - One word many times? Many words one time?\n        - Style\n\n## Which \"Re-\" Words Are Most Frequent: Code\n\n- Keyword Frequency\n    - Have computer count all words that start with \"re-\" in poet's corpus \n    - Stemming\n        - Remove inflected endings to condense different versions of same term to a common stem\n        - \"re-enter: 1\", \"re-entering: 1\", \"re-entered: 1\" --> \"re-ent: 3\"\n        - Improves IDing significant concepts\n    - Visualize results in bar chart\n\n```{python}\n \n# import software libraries / dependencies\nimport nltk\nimport os\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('punkt')  # Download NLTK tokenizer data\n\nstemmer = SnowballStemmer(\"english\")  # Initialize SnowballStemmer for English\n\n# create function to preprocess and process files of each directory\ndef process_directory(corpus_directory):   \n    corpus = []\n\n    # Get the directory name as the label\n    label = os.path.basename(corpus_directory)\n\n    # Step 1: Get the text of corpus from files\n    for filename in os.listdir(corpus_directory):\n        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n            text = file.read()\n            corpus.append(text)\n\n    # Step 2: Tokenize the text blob into individual words\n    tokenized_corpus = [nltk.word_tokenize(text) for text in corpus]\n\n    # Step 3: Standardize (lower) case, find words that start with re-, stem those words, retain them\n    stemmed_corpus = []\n    for tokens in tokenized_corpus:\n        stemmed_tokens = [stemmer.stem(word.lower()) for word in tokens if re.match(r'\\b(re-)\\w+', word.lower())]\n        stemmed_corpus.append(stemmed_tokens)\n\n    # Step 6: Count the frequency of each re- word\n    word_counts = Counter(word for tokens in stemmed_corpus for word in tokens)\n\n    # Step 7: Display the most frequent words\n    # most_common_re_words = word_counts.most_common(20)  # Set the desired number of top words\n    # for word, count in most_common_re_words:\n    #    print(f'The poetry of {label[:-5]} uses {word}: {count} times')\n\n    # Step 8: Plot words and counts in a bar chart\n    plt.figure(figsize=(10, 5))\n    top_words, top_counts = zip(*word_counts.most_common(20))\n    plt.bar(top_words, top_counts)\n    plt.title(f'Frequency of Stemmed Re- Words in {label[:-5].capitalize()}\\'s Poetry')\n    plt.xticks(rotation=65)\n    plt.show()\n\n# Directories to process\ncorpus_directories = [\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n    # Add more directories here\n]\n\n# Process each directory\nfor directory in corpus_directories:\n    process_directory(directory)\n```\n\n## Results: Take Aways\n\n- Comparison of \"Re-\" Use\n\n|                    | Low Vocabulary | High Vocabulary |\n|------------------- | ---------------| ----------------|\n| **High Frequency** | Swinburne      |                 |\n| **Low Frequency**  | Rossetti       | Hardy, Field    |\n\n## Results: Take Aways\n\n- Swinburne: \"Re-Risen\"\n    - 12x (80% of S's \"Re-\" Words)\n        - \"re-risen: form: alliterative \n        - \"re-risen\": content: interested in ressurection?\n\n\n## Results: Take Aways\n\n- DG Rossetti's \"Re-Born\"\n    - Hapax Legomenon\n    - But hyphenated prefixes (e.g. \"a-heap\", \"to-night\") and compound words (e.g. \"cukoo-throb\", \"forest-boughs\") are common (1.22%) in DGR's poetry\n    - \"born\": 48x and \"birth\": 46x \n        - \"ressurect*\", like \"re-born,\" appears only once\n \n    - Hypotheses:\n        - uninterested in signifying repetition through \"re-\"\n        - more interested in birth than rebirth?\n\n## Results: Take Aways\n\n- Hardy and Field: \"Re-Illume\" in Context\n    - Extremely rare\n    - Chiefly poetic\n    - 1758 - present\n    - Hardy: 2x\n        - \"Two Rosalinds\": *Time's Laughingstocks and Other Verses* (1909)\n        - \"For Life I had never cared greatly\": *Moments of Vision and Miscellaneous Verses* (1917)\n    - Field: 1x\n        - [\"XXII: Sleeping together: Sleep\"](https://editions.covecollective.org/edition/whym-chow-flame-love/whym-chow-flame-love#chapter22): *Whym Chow: Flame of Love* (1906, 1914)\n\n## {background-iframe=\"https://books.google.com/ngrams/graph?content=reillumed%2Breillume%2Breilluming%2Breillumes&year_start=1779&year_end=2019&corpus=en-GB-2019&smoothing=3\" background-interactive=\"true\"}\n\n## Which Words Adjoin \"Re-\" Words?\n\n- J. R. Firth (Linguist): \"You shall know a word by the company it keeps\" (1957)\n- \"re-\" words: IDing (frequently) co-occuring terms can better clarify their meaning\n\n## Which Words Adjoin \"Re-\" Words?\n\n- Bigrams\n    - Bigrams: two consecutive words\n        - e.g. \"She used the olive oil.\": \"She used\": 1, \"used the\": 1, \"the olive\": 1, \"olive oil\": 1\n    - Have computer ID / count bigrams of re- words: re- word + consecutive word\n    - remove stopwords (meaningless function words) to better reveal associated concepts\n\n```{python}\n# import software dependencies / libraries\nimport nltk\nimport os\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom collections import Counter\nimport string\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('stopwords')\n\n# Directories to process, Aliases \ncorpus_directories = {\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n}\n\n# RegEx to match words starting with \"re-\"\nre_word_pattern = r'\\b(re-\\w+)'\n\n# Dictionary to store directory/poet: bigram frequencies \nbigram_frequencies = {alias: Counter() for alias in corpus_directories.values()}\n\n# Define the punctuation to remove (including curly quotation marks)\npunctuation_to_remove = string.punctuation.replace('-', '') + \"‘’“”\"\n\n# Preprocess and Process each directory\nfor corpus_directory, alias in corpus_directories.items():\n    # Get the text from the text files in the corpus\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n\n                # Tokenize the text\n                tokens = nltk.word_tokenize(text)\n\n                # Remove punctuation except hyphens and standardize case\n                translator = str.maketrans('', '', punctuation_to_remove)\n                preprocessed_text = ' '.join(tokens).translate(translator).lower()\n\n                # Remove stopwords\n                stop_words = set(stopwords.words(\"english\"))\n                filtered_tokens = [word for word in preprocessed_text.split() if word not in stop_words]\n\n                # Find bigrams\n                bigrams = list(ngrams(filtered_tokens, 2))\n\n                # Filter bigrams to include only those starting with \"re-\"\n                re_bigrams = [bigram for bigram in bigrams if re.match(re_word_pattern, bigram[0])]\n\n                # Count the bigrams\n                bigram_frequency = Counter(re_bigrams)\n\n                # Add the bigram frequencies as values of the current directory/poet as keys\n                bigram_frequencies[alias].update(bigram_frequency)\n\n# Print the sorted bigram frequencies per directory/poet\nfor alias, frequencies in bigram_frequencies.items():\n    print(f\"{alias}:\")\n    for bigram, frequency in sorted(frequencies.items(), key=lambda x: x[0]):  # Sort alphabetically\n        print(f\"{bigram}: {frequency}\")\n    print()\n```\n\n## Results: Take Aways\n\n- Swinburne\n    - \"re-__\" : \"life\", \"dead\" (2x), \"dust\", \"mortal\"\n        - re- associated with life + death, mortality, etc.\n    - \"re-risen\": \"prison\": rhyme\n- Hardy\n    - \"re-____\": \"time\" (2x), \"semipiternal\" (everlasting), \"killing\", \"death\", \"lives\" \"new\", \"olden\"\n        - re- associated with time, eternity / mortality, etc.\n- Field\n    - \"re-appear\": \"fade\", \"re-light\": \"tarnished\"\n        - re- used with opposites\n\n## Do \"re-\" words and \"re\" words co-occur in Hardy's poetry?\n\n- Does Hardy juxtapose \"re-\" words and \"re\" words? \n\n## Do \"re-\" words and \"re\" words co-occur in Hardy's poetry?\n\n- Have Python return Hardy's sentences that contain both a word(s) that start with \"re-\" and a word(s) that start with \"re\"\n\n```{python}\nimport os\nimport nltk\nimport re\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n# Define regular expressions\nre_pattern = r'\\b(?:[Rr]e|[Rr]E)\\w+\\b'  # Matches \"re\" followed by one or more letters\nre_hyphen_pattern = r'\\b(?:[Rr]e-|[Rr]E-)\\w+\\b'  # Matches \"re-\" followed by one or more word characters\n\n# \\b matches a word boundary.\n# (?:[Rr]e|[Rr]E) is a non-capturing group that matches either \"re\" or \"RE,\" regardless of case.\n# \\w+ matches one or more word characters following \"re-\" or \"RE-.\"\n\ndef matches_pattern(word, pattern):\n    return re.search(pattern, word, re.IGNORECASE) is not None\n\n# Function to replace matched words with bolded HTML tags\ndef bold_matched_words(match, pattern):\n    return f'**{match.group()}**'\n\n# Directory containing text files\ntext_directory = \"/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP\"\n\n# Iterate through text files in the directory\nfor filename in os.listdir(text_directory):\n    if filename.endswith(\".txt\"):\n        with open(os.path.join(text_directory, filename), \"r\", encoding=\"utf-8\") as file:\n            text = file.read()\n            sentences = sent_tokenize(text)\n            \n            for sentence in sentences:\n                words = word_tokenize(sentence)\n                has_re = any(matches_pattern(word, re_pattern) for word in words)\n                has_re_hyphen = any(matches_pattern(word, re_hyphen_pattern) for word in words)\n                \n                if has_re and has_re_hyphen:\n                    sentence_with_bold = re.sub(re_pattern, lambda x: bold_matched_words(x, re_pattern), sentence)\n                    sentence_with_bold = re.sub(re_hyphen_pattern, lambda x: bold_matched_words(x, re_hyphen_pattern), sentence_with_bold)\n                    print(sentence_with_bold)\n                    print(\"\\n\" * 2)\n```\n\n## Results: Take Aways\n\n- 30% (6 / 20) of sentences containing \"re-\" words in Hardy's poetry also contain \"re\" words\n- Hardy: \"re-\" words co-occur with \"re\" words\n\n- Hardy's style\n    - echo in form: visual and audial rhyme\n    - echo in content: re(-) returns\n\n## When Were \"Re-\" Words More / Less Frequent\n\n- ID when (in poet's career) \"re-\" words were used more / less often\n    - Why did \"re-\" wax / wane then?\n- In which book(s) is re- frequent / infrequent? Why?\n\n## When Were \"Re-\" Words More / Less Frequent\n\n- Time Series / Term Frequency over Time\n    - Y axis: re- word percentage of book\n    - X axis: publication year of book\n\n```{python}\n# Import software libraries / dependencies\nimport nltk\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport string\n\nnltk.download('punkt')  # Download NLTK tokenizer data\n\n# Define a function to count words that start with \"re-\"\ndef count_re_words(text):\n    words = nltk.word_tokenize(text)\n    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n\n# Define a function to remove all punctuation except hyphens\ndef remove_punctuation_except_hyphens(text):\n    translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n    return text.translate(translator)\n\n# Specify the parent directory containing multiple text file directories\nparent_directory = '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/'\n\n# Soecific irectories to process\nall_directory_names = [\n    'swinburne/swinburne_noBP',\n    'hardy/hardy_noBP',\n    'field/field_NoBP',\n    'rossetti_dg/rossetti_dg_NoBP',\n]\n\n# Initialize a dictionary to store year-wise percentages\nyear_percentages = {}\n\n# Process each directory\nfor directory_name in all_directory_names:\n    # Construct the full path to the current directory\n    text_files_directory = os.path.join(parent_directory, directory_name)\n\n    if os.path.isdir(text_files_directory):\n        # Initialize a dictionary to store year-wise percentages for the current directory\n        directory_percentages = {}\n\n        # Initialize a dictionary to store file names for the current directory\n        file_names = {}\n\n        # Process each text file in the current directory\n        for filename in os.listdir(text_files_directory):\n            if filename.endswith('.txt'):\n                # Extract the year from the filename using a regular expression\n                year_match = re.match(r'(\\d{4})_', filename)\n                if year_match:\n                    year = int(year_match.group(1))\n                    with open(os.path.join(text_files_directory, filename), 'r', encoding='utf-8') as file:\n                        text = file.read()\n                        # Remove punctuation except hyphens\n                        text = remove_punctuation_except_hyphens(text)\n                        # normalize counts\n                        total_words = len(nltk.word_tokenize(text))\n                        re_word_count = count_re_words(text)\n                        percentage_re_words = (re_word_count / total_words) * 100\n                        \n                        #add to dictionary: values: counts to keys: years\n                        directory_percentages[year] = percentage_re_words\n                        \n                        # Extract the text between underscores in the filename\n                        file_name_parts = filename.split('_')\n                        if len(file_name_parts) > 2:\n                            file_name = '_'.join(file_name_parts[1:-1])\n                        else:\n                            file_name = file_name_parts[1]\n                        \n                        #add to dictionary: values: filenames to keys: years \n                        file_names[year] = file_name  # Store the extracted file name\n\n        # Sort the dictionary by keys (years) for the current directory\n        sorted_directory_percentages = {year: directory_percentages[year] for year in sorted(directory_percentages)}\n\n        # Store the results for the current directory\n        year_percentages[directory_name] = {\n            'percentages': sorted_directory_percentages,\n            'file_names': file_names  # Store file names for this directory\n        }\n\n# Step 2: Plot the keys (years) and values (percentages) in a line graph for each directory\nplt.figure(figsize=(15, 8))\n\nfor directory_name, data in year_percentages.items():\n    percentages = data['percentages']\n    file_names = data['file_names']\n    years = list(percentages.keys())\n    percentages = list(percentages.values())\n    \n    # Plot the data points\n    plt.plot(years, percentages, marker='o', linestyle='-', label=directory_name[:-5])\n    \n    # Add annotations for each data point if the value is greater than 0\n    for year, percentage in zip(years, percentages):\n        if percentage > 0:\n            file_name = file_names[year]\n            annotation = f\"{file_name}\"\n            plt.annotate(annotation, (year, percentage), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n\nplt.ylim(0, 0.0525)\nplt.xlabel('Year')\nplt.ylabel('Percentage of Words Starting with \"Re-\"')\nplt.title('When Were \"Re-\" Words Used in Field\\'s, Hardy\\'s, DG Rossetti\\'s, and Swinburne\\'s Poetry?')\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.legend()  # Show legend indicating directory names\n\n# Display the line graph\nplt.tight_layout()\nplt.show()\n```\n\n## Results: Take Aways\n\n- Swinburne\n    - \"Re-\": of interest in mid career, especially in *A Century of Roundels* (1883)\n        - \"Re-\": prefix (repetition) apt for Roundel's repetitive form\n- Field\n    - \"Re-\": of interest in *Whym Chow: Flame of Love* (1914)\n        - \"Re-\": prefix (\"again-ness,\" return, and memory) apt for an elegy \n\n## Are \"Re-\" Words Used Positively or Negatively?\n\n- Do \"re-\" words appear more often in contexts that are positive or negative?\n- Thematics: what do \"re-\" words + repetition connote?\n- Stylistics: poet's emotional associations of \"re-\" words\n\n## Are \"Re-\" Words Used Positively or Negatively?\n\n- Sentiment Analysis\n    - determines (part of) a text's emotional tone (positive / negative / neutral)\n    - aggregate sentiment scores of all sentences with re- words\n    - compare to sentiment score of poet's corpus (norm)\n- Sentiment Analyzer: VADER (Valence Aware Dictionary and sEntiment Reasoner)\n    - lexicon-based approach\n        - considers intensifiers, valence shifting, & punctuation\n        - struggles with sarcasm\n```{python}\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport os\nimport re\nimport matplotlib.pyplot as plt\n\n# Initialize the VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Function to calculate aggregate sentiment for a collection of sentences\ndef calculate_aggregate_sentiment(sentences):\n    positive_score = 0\n    negative_score = 0\n    neutral_score = 0\n    total_sentences = 0\n\n    for sentence in sentences:\n        sentiment = sia.polarity_scores(sentence)\n        positive_score += sentiment['pos']\n        negative_score += sentiment['neg']\n        neutral_score += sentiment['neu']\n        total_sentences += 1\n\n    if total_sentences > 0:\n        avg_positive_score = positive_score / total_sentences\n        avg_negative_score = negative_score / total_sentences\n        avg_neutral_score = neutral_score / total_sentences\n\n        if avg_positive_score > avg_negative_score:\n            overall_sentiment = \"Positive\"\n        elif avg_positive_score < avg_negative_score:\n            overall_sentiment = \"Negative\"\n        else:\n            overall_sentiment = \"Neutral\"\n\n        return {\n            \"Total Sentences Analyzed\": total_sentences,\n            \"Average Positive Score\": avg_positive_score,\n            \"Average Negative Score\": avg_negative_score,\n            \"Average Neutral Score\": avg_neutral_score,\n            \"Overall Sentiment\": overall_sentiment,\n        }\n\n# Specify the directories you want to process with aliases\ncorpus_directories = {\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n    # Add more directories here\n}\n\n# Create lists to store poet names and their corresponding score differences\npoet_names = []\nscore_differences = []\n\n# Process each directory\nfor corpus_directory, alias in corpus_directories.items():\n    sentences = []\n\n    # Read and preprocess the text files in the corpus\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n                sentences += nltk.sent_tokenize(text)\n\n    # Calculate aggregate sentiment\n    results = calculate_aggregate_sentiment(sentences)\n\n    # Calculate the difference between positive and negative scores\n    positive_score = results[\"Average Positive Score\"]\n    negative_score = results[\"Average Negative Score\"]\n    score_difference = positive_score - negative_score\n\n    poet_names.append(alias)\n    score_differences.append(score_difference)\n\n# Sort the poet names and score differences by score differences in descending order\nsorted_poet_names, sorted_score_differences = zip(*sorted(zip(poet_names, score_differences), key=lambda x: x[1], reverse=True))\n\n# Create a bar chart of the score differences with poet names on the x-axis\nplt.figure(figsize=(10, 6))\nplt.bar(sorted_poet_names, sorted_score_differences, color='skyblue')\nplt.xlabel(\"Poet\")\nplt.ylabel('Difference between Positive and Negative Scores')\nplt.title('Whose Poetry Is Most Positive?')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y')\nplt.show()\n```\n\n```{python}\n#| echo: true\n#| output-location: slide\n\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport os\n\n# Initialize the VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Function to calculate aggregate sentiment for a collection of sentences\ndef calculate_aggregate_sentiment(sentences):\n    positive_score = 0\n    negative_score = 0\n    neutral_score = 0\n    total_sentences = 0\n\n    for sentence in sentences:\n        sentiment = sia.polarity_scores(sentence)\n        positive_score += sentiment['pos']\n        negative_score += sentiment['neg']\n        neutral_score += sentiment['neu']\n        total_sentences += 1\n\n    if total_sentences > 0:\n        avg_positive_score = positive_score / total_sentences\n        avg_negative_score = negative_score / total_sentences\n        avg_neutral_score = neutral_score / total_sentences\n\n        if avg_positive_score > avg_negative_score:\n            overall_sentiment = \"Positive\"\n        elif avg_positive_score < avg_negative_score:\n            overall_sentiment = \"Negative\"\n        else:\n            overall_sentiment = \"Neutral\"\n\n        return {\n            \"Total Sentences Analyzed\": total_sentences,\n            \"Average Positive Score\": avg_positive_score,\n            \"Average Negative Score\": avg_negative_score,\n            \"Average Neutral Score\": avg_neutral_score,\n            \"Overall Sentiment\": overall_sentiment,\n        }\n    else:\n        return {\"No sentences with 're-' words found for analysis.\"}\n\n# Specify the directories you want to process with aliases\ncorpus_directories = {\n \n        '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n       # Add more directories here\n}\n\n# Process each directory\nfor corpus_directory, alias in corpus_directories.items():\n    sentences = []\n\n    # Read and preprocess the text files in the corpus\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n                sentences += nltk.sent_tokenize(text)\n\n    # Filter sentences with words starting with \"re-\"\n    sentences_with_re = [sentence for sentence in sentences if any(word.lower().startswith(\"re-\") for word in nltk.word_tokenize(sentence))]\n\n    # Calculate aggregate sentiment for the filtered sentences\n    results = calculate_aggregate_sentiment(sentences_with_re)\n\n    # Print results for the current directory\n    print(f\"{alias}\")\n    for key, value in results.items():\n        print(f\"{key}: {value}\")\n    print()\n```\n\n## Results: Take Aways\n\n|                          | Poetry: More Positive | Poetry: More Negative |\n|--------------------------| ----------------------| ----------------------|\n| **\"Re-\": More Positive** | Field                 | Rossetti              |\n| **\"Re-\": More Negative** | Swinburne             | Hardy                 |\n\n## Wrap Up + Thank You!\n\n- Contact Info\n    - Adam Mazel\n    - Digital Publishing Librarian\n    - Indiana University Bloomington\n    - [amazel@iu.edu](mailto:amazel@iu.edu)"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":true,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en-US","fig-responsive":true,"quarto-version":"1.3.450","auto-stretch":true,"title":"Thirteen* Digital Ways of Looking at Re- in Victorian Poetry","date":"11 November 2023","author":"Adam Mazel, Digital Publishing Librarian","institute":"Scholarly Communication Department, IUB Libraries","preloadIframes":true,"output-location":"slide","scrollable":true,"chalkboard":true,"overview":true,"touch":true,"progress":true,"slideNumber":true,"previewLinks":true,"transition":"slide","keywords":["Digital Humanities","Text Mining","Victorian Poetry","re"]}}},"projectFormats":["revealjs"]}
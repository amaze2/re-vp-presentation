{"title":"Text Mining \"Re-\" in Victorian Poetry","markdown":{"yaml":{"title":"Text Mining \"Re-\" in Victorian Poetry","date":"11 November 2023","author":"Adam Mazel, Digital Publishing Librarian","institute":"Scholarly Communication Department, IUB Libraries"},"headingText":"Why \"Re-\" in VP?","headingAttr":{"id":"","classes":["smaller"],"keyvalue":[["visibility","hidden"]]},"containsRefs":false,"markdown":"\n\n\n:::: {.columns}\n\n::: {.column}\n- NAVSA Conference 2023: \"**<u>Re</u>**vision, **<u>Re</u>**turn, **<u>Re</u>**form\"\n    - Panel: \"Re: Re: Victorian Poetry\"\n        - Adela Pinch, Professor of English, University of Michigan\n        - Emily Harrington, Assoc. Prof. of English, UC Boulder\n        - Naomi Levine, Asst. Prof. of English, Yale University\n        - Me!\n:::\n\n::: {.column}\n![](images/navsa.png){fig-alt=\"2023 NAVSA Conference Banner\" width=\"85%\"}\n:::\n\n::: aside\nNAVSA 2023 Conference banner excerpted from [conference website](https://www.navsa2023.com/)\n:::\n\n::::\n\n## Methodology \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- \"Re-\" :handshake: Text Mining \t\n- Python\n    - Keyword Frequency\n    - Key Word in Context (KWIC)\n    - Term Frequency over Time\n    - Sentiment Analysis\n- Exploratory Data Analysis\n:::\n\n::: {.column width=\"40%\"}\n![](images/python.png){fig-alt=\"python programming language icon\"}\n![](images/plot.png){fig-alt=\"python scatterplot of recipe ingredient frequency before / after 1900\"}\n:::\n\n::: aside\nPython logo used with [permission](https://www.python.org/community/logos/)\n:::\n\n::::\n\n## Data: Dante Gabriel Rossetti\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- *Poems* (1870)\n- *Poems: A New Edition* (1881)\n- *Ballads and Sonnets* (1881)\n:::\n\n::: {.column width=\"40%\"}\n![Self Portrait, 1861](images/dgrossetti.png){fig-alt=\"self-portrait of DG Rossetti\"}\n:::\n\n::: aside\nImage from the [Rossetti Archive](http://www.rossettiarchive.org/docs/s438.rap.html) in public domain in US\n:::\n\n::::\n\n## Data: Algernon Charles Swinburne {.smaller}\n\n:::: {.columns}\n\n::: {.column} \n- *Atalanta in Calydon* (1865)\n- *Poems and Ballads* (1866)\n- *Songs Before Sunrise* (1871)\n- *Songs of Two Nations* (1875)\n- *Erechtheus* (1876)\n- *Poems and Ballads, Second Series* (1878)\n- *Songs of the Springtides* (1880)\n- *Studies in Song* (1880)\n:::\n\n::: {.column}\n- *The Heptalogia, or the Seven against Sense. A Cap with Seven Bells* (1880)\n- *Tristram of Lyonesse* (1882)\n- *A Century of Roundels* (1883)\n- *A Midsummer Holiday and Other Poems* (1884)\n- *Poems and Ballads, Third Series* (1889)\n- *Astrophel and Other Poems* (1894)\n- *The Tale of Balen* (1896)\n- *A Channel Passage and Other Poems* (1904)\n:::\n\n::::\n\n## Data: Michael Field {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- *Long Ago* (1889)\n- *Sight and Song* (1892)\n- *Underneath the Bough* (1893)\n- *Wild Honey from Various Thyme* (1908)\n- *Poems of Adoration* (1912)\n- *Mystic Trees* (1913)\n- *Whym Chow: Flame of Love* (1914)\n:::\n\n::: {.column width=\"40%\"}\n![Katherine Bradley & Edith Cooper, aka Michael Field](images/field.jpg){fig-alt=\"photo of Katherine Bradley & Edith Cooper\"}\n:::\n\n::: aside\nImage from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Katherine_Harris_Bradley_%26_Edith_Emma_Cooper_(2).jpg) in public domain in US\n:::\n\n::::\n\n## Data: Thomas Hardy {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- *Wessex Poems and Other Verses* (1898)\n- *Poems of the Past and the Present* (1901)\n- *Time's Laughingstocks and Other Verses* (1909)\n- *Satires of Circumstance* (1914)\n- *Moments of Vision* (1917)\n- *Late Lyrics and Earlier with Many Other Verses* (1922)\n- *Human Shows, Far Phantasies, Songs and Trifles* (1925)\n:::\n\n::: {.column width=\"40%\"}\n![](images/hardy.jpg){fig-alt=\"headshot of Thomas Hardy\" width=\"63%\"}\n:::\n\n::: aside\nImage from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Thomashardy_restored.jpg) in public domain in US\n:::\n\n::::\n\n## Who Uses \"Re-\" Words More / Less?\n\n- Keyword Frequency Analysis\n    - What percent of each poet’s corpus is composed of words with the prefix \"re-\"?\n    - Compare \"Re-\" Word Percentages\n\n```{python}\n# import software libraries\nimport nltk\nimport os\nimport matplotlib.pyplot as plt\n\n# Download NLTK tokenizer data\nnltk.download('punkt') \n\n# Define a function that tokenizes a text and then counts how many of its words start with \"re-\"\ndef count_re_words(text):\n    words = nltk.word_tokenize(text)\n    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n\n# Specify the directory paths for the two poets' corpora\ncorpus_directories = {\n    'swinburne': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n    'hardy': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n    'michael field': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n    'dg rossetti': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n}\n\n# Initialize dictionary to store the results\npercentage_re_words = {}\n\n# create an empty list for each poet/directory\nfor poet, corpus_directory in corpus_directories.items():\n    corpus = []\n    \n    # Read the text files in the poet's corpus and add them to the apt list above\n    for filename in os.listdir(corpus_directory):\n        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n            text = file.read()\n            corpus.append(text)\n\n    # Apply fuction above to tokenize + count the \"re-\" words in each text of the poet's corpus and store count\n    re_word_count = sum(count_re_words(text) for text in corpus)\n\n    # Calculate the corpus word count by taking each text, tokenizing it, counting the number of tokens, and then adding them to create a total word count \n    total_words = sum(len(nltk.word_tokenize(text)) for text in corpus)\n    # Calculate the percentage of \"re-\" words in the poet's corpus by dividing the re- word count by the total word count and multiply by 100\n    percentage_re_words[poet] = (re_word_count / total_words) * 100\n\n# Sort the results from largest to smallest\nsorted_results = sorted(percentage_re_words.items(), key=lambda x: x[1], reverse=True)\n\n# Extract poets and percentages for plotting\npoets, percentages = zip(*sorted_results)\n\n# Step 4: visualize the results in a bar chart \nplt.figure(figsize=(8, 6))\nplt.bar(poets, percentages, color=['blue', 'orange'])\nplt.ylabel('Percentage (%)')\nplt.title('Whose Poetry is More Composed of Words that Start with \"Re-\"?')\n\n# Set the y-axis limit based on the largest percentage\nylim_percentage = max(percentages) * 2  # Adjusted for better visualization\nplt.ylim(0, ylim_percentage)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Display the bar chart with the poet's names angled to avoid overlap\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n## Which \"Re-\" Words Are Most Frequent?\n\n- Keyword Frequency Analysis\n    - Which \"re-\" words are used and how often?\n\n```{python}\n# import software libraries / dependencies\nimport nltk\nimport os\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom nltk.stem import SnowballStemmer\n\n# Download NLTK tokenizer data\nnltk.download('punkt')  \n\n# Initialize Stemmer for English\nstemmer = SnowballStemmer(\"english\")  \n\n# create function to preprocess and process files of each directory: create an empty list for each directory/poet\ndef process_directory(corpus_directory):   \n    corpus = []\n\n    # Get the directory (poet's) name to use as the label\n    label = os.path.basename(corpus_directory)\n\n    # Step 1: Read the text of corpus from files and add it to apt (poet's) list, created above\n    for filename in os.listdir(corpus_directory):\n        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n            text = file.read()\n            corpus.append(text)\n\n    # Step 2: Tokenize each text into individual words\n    tokenized_corpus = [nltk.word_tokenize(text) for text in corpus]\n\n    # Step 3: find words that when lower cased start with re-, standardize (lower) their case, stem them, retain them\n    stemmed_corpus = []\n    for tokens in tokenized_corpus:\n        stemmed_tokens = [stemmer.stem(word.lower()) for word in tokens if re.match(r'\\b(re-)\\w+', word.lower())]\n        stemmed_corpus.append(stemmed_tokens)\n\n    # Step 6: Count the frequency of each re- word\n    word_counts = Counter(word for tokens in stemmed_corpus for word in tokens)\n\n    # Step 7: Display the most frequent words\n    # most_common_re_words = word_counts.most_common(20)  # Set the desired number of top words\n    # for word, count in most_common_re_words:\n    #    print(f'The poetry of {label[:-5]} uses {word}: {count} times')\n\n    # Step 8: Plot top 20 most frequent words and counts in a bar chart, use lable to title plot\n    plt.figure(figsize=(10, 5))\n    top_words, top_counts = zip(*word_counts.most_common(20))\n    plt.bar(top_words, top_counts)\n    plt.title(f'Frequency of Stemmed Re- Words in {label[:-5].capitalize()}\\'s Poetry')\n    plt.xticks(rotation=65)\n    plt.show()\n\n# Directories to process\ncorpus_directories = [\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n    # Add more directories here\n]\n\n# Process each directory\nfor directory in corpus_directories:\n    process_directory(directory)\n```\n\n## Field's and Hardy's \"Re-Illume\" in Context {.smaller}\n\n- Re-illume\n    - Extremely rare\n    - Chiefly poetic\n    - 1758 &ndash; present\n\n- Field: 1x\n    - [\"XXII: Sleeping together: Sleep\"](https://editions.covecollective.org/edition/whym-chow-flame-love/whym-chow-flame-love#chapter22): *Whym Chow: Flame of Love* (1906, 1914)\n- Hardy: 2x\n    - \"Two Rosalinds\": *Time's Laughingstocks and Other Verses* (1909)\n    - \"For Life I had never cared greatly\": *Moments of Vision and Miscellaneous Verses* (1917)\n\n## {background-iframe=\"https://books.google.com/ngrams/graph?content=reillumed%2Breillume%2Breilluming%2Breillumes&year_start=1779&year_end=2019&corpus=en-GB-2019&smoothing=3\" background-interactive=\"true\"}\n\n## Do \"Re-\" Words and \"Re\" Words Co-occur in Hardy's Poetry?\n\n- Key (Re-) Word in Context (KWIC)\n- Have Python return Hardy's sentences that contain word(s) that start with \"re-\" and word(s) that start with \"re\"\n\n```{python}\n# import libraries\nimport os # open directories on local machine\nimport nltk # nlp\nimport re # reg ex\nfrom nltk.tokenize import sent_tokenize, word_tokenize # break text blob into individual sentences and words\n\n# Define regular expressions\nre_pattern = r'\\b(?:[Rr]e|[Rr]E)\\w+\\b'  # Matches \"re\" or \"Re\" or \"rE\" or \"RE\" followed by one or more letters (not hyphen)\nre_hyphen_pattern = r'\\b(?:[Rr]e-|[Rr]E-)\\w+\\b'  # Matches \"re-\" or \"Re-\" or \"rE-\" or \"RE-\" followed by one or more letters\n# \\b matches a word boundary.\n# (?:[Rr]e|[Rr]E) is a non-capturing group that matches either \"re\" or \"Re\" or \"rE\" or \"RE\" \n# \\w+ matches one or more word characters following \"re\" or \"Re\" or \"rE\" or \"RE\" \n\ndef matches_pattern(word, pattern):\n    return re.search(pattern, word, re.IGNORECASE) is not None\n\n# Define function to surround matched words with asterisks\ndef bold_matched_words(match, pattern):\n    return f'**{match.group()}**'\n\n# Directory containing text files\ntext_directory = \"/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP\"\n\n# Iterate through text files in the directory, open text files, read in text, break up text into sentences, break up sentences into words, id those sentences with re/re- words, if sentence has re/re-, add asterisks around re-/re words and print sentence\nfor filename in os.listdir(text_directory):\n    if filename.endswith(\".txt\"):\n        with open(os.path.join(text_directory, filename), \"r\", encoding=\"utf-8\") as file:\n            text = file.read()\n            sentences = sent_tokenize(text)\n            \n            for sentence in sentences:\n                words = word_tokenize(sentence)\n                has_re = any(matches_pattern(word, re_pattern) for word in words)\n                has_re_hyphen = any(matches_pattern(word, re_hyphen_pattern) for word in words)\n                \n                if has_re and has_re_hyphen:\n                    sentence_with_bold = re.sub(re_pattern, lambda x: bold_matched_words(x, re_pattern), sentence)\n                    sentence_with_bold = re.sub(re_hyphen_pattern, lambda x: bold_matched_words(x, re_hyphen_pattern), sentence_with_bold)\n                    print(sentence_with_bold)\n                    print(\"\\n\" * 2)\n```\n\n## When Are \"Re-\" Words More / Less Frequent?\n\n- Time Series / Term Frequency over Time\n    - Y axis: re- word percentage of book\n    - X axis: publication year of book\n\n```{python}\n# Import software libraries\nimport nltk # nlp\nimport os # interact with directories on local machine\nimport re # reg ex\nimport matplotlib.pyplot as plt # data visualization\nimport string # punctuation removal\n\n# Download NLTK tokenizer data\nnltk.download('punkt')  \n\n# Define a function that takes each text, tokenizes it into individual words, and count words that when lowercased start with \"re-\"\ndef count_re_words(text):\n    words = nltk.word_tokenize(text)\n    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n\n# Define a function to remove all punctuation except hyphens\ndef remove_punctuation_except_hyphens(text):\n    translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n    return text.translate(translator)\n\n# Specify the parent directory containing multiple text file directories\nparent_directory = '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/'\n\n# Soecific irectories to process\nall_directory_names = [\n    'swinburne/swinburne_noBP',\n    'hardy/hardy_noBP',\n    'field/field_NoBP',\n    # 'rossetti_dg/rossetti_dg_NoBP',\n]\n\n# Initialize a dictionary to store year-wise percentages\nyear_percentages = {}\n\n# Process each directory\nfor directory_name in all_directory_names:\n    # Construct the full path to the current directory\n    text_files_directory = os.path.join(parent_directory, directory_name)\n\n    # Initialize two dictionaries: one to store year-wise percentages for the current directory and one to store file names for the current directory\n    if os.path.isdir(text_files_directory):\n        directory_percentages = {}\n        file_names = {}\n\n        # Process each text file in the current directory\n        for filename in os.listdir(text_files_directory):\n            if filename.endswith('.txt'):\n                # Extract the year from the filename using a regular expression\n                year_match = re.match(r'(\\d{4})_', filename)\n                if year_match:\n                    year = int(year_match.group(1))\n                    # read in text of each book of poetry\n                    with open(os.path.join(text_files_directory, filename), 'r', encoding='utf-8') as file:\n                        text = file.read()\n                        # Remove punctuation except hyphens\n                        text = remove_punctuation_except_hyphens(text)\n                        # tokenize text, count the total number of words, count the number of re- words, normalize counts\n                        total_words = len(nltk.word_tokenize(text))\n                        re_word_count = count_re_words(text)\n                        percentage_re_words = (re_word_count / total_words) * 100\n                        \n                        #add to dictionary: value: count, key: publication year\n                        directory_percentages[year] = percentage_re_words\n                        \n                        # Extract the text between underscores in the filename\n                        file_name_parts = filename.split('_')\n                        if len(file_name_parts) > 2:\n                            file_name = '_'.join(file_name_parts[1:-1])\n                        else:\n                            file_name = file_name_parts[1]\n                        \n                        #add to dictionary: value: filename, key: years\n                        file_names[year] = file_name  # Store the extracted file name\n\n        # Sort the dictionary by keys (years) for the current directory\n        sorted_directory_percentages = {year: directory_percentages[year] for year in sorted(directory_percentages)}\n\n        # Store the results for the current directory\n        year_percentages[directory_name] = {\n            'percentages': sorted_directory_percentages,\n            'file_names': file_names  # Store file names for this directory\n        }\n\n# Step 2: Plot the keys (years) and values (percentages) in a line graph for each directory\nplt.figure(figsize=(10, 8))\n\nfor directory_name, data in year_percentages.items():\n    percentages = data['percentages']\n    file_names = data['file_names']\n    years = list(percentages.keys())\n    percentages = list(percentages.values())\n    \n    # Plot the data points\n    plt.plot(years, percentages, marker='o', linestyle='-', label=directory_name[:-5])\n    \n    # Add annotations for each data point if the value is greater than 0\n    for year, percentage in zip(years, percentages):\n        if percentage > 0:\n            file_name = file_names[year]\n            annotation = f\"{file_name}\"\n            plt.annotate(annotation, (year, percentage), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n\nplt.ylim(0, 0.0525)\nplt.xlabel('Year')\nplt.ylabel('Percentage of Words Starting with \"Re-\"')\nplt.title('When Are \"Re-\" Words Used in Field\\'s, Hardy\\'s, and Swinburne\\'s Poetry?')\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.legend()  # Show legend indicating directory names\n\n# Display the line graph\nplt.tight_layout()\nplt.show()\n```\n\n## Are \"Re-\" Words Used Positively or Negatively?\n\n- Sentiment Analysis\n    - determines a text's emotional tone (positive / negative / neutral)\n\n```{python}\n# import software libraries\nimport nltk #nlp\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer # VADER sentiment analyzer\nimport os # enable engagement with directories and files on local machine\n\n# Initialize the VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Function to calculate aggregate sentiment (positive / neutral / negative) for a collection of sentences\n# start counts at zero\ndef calculate_aggregate_sentiment(sentences):\n    positive_score = 0\n    negative_score = 0\n    neutral_score = 0\n    total_sentences = 0\n\n# for each sentence, calculate sentiment polarity score: if score is positive, add positive score to positive score count; lastly keep a running count of the number of sentences analyzed\n    for sentence in sentences:\n        sentiment = sia.polarity_scores(sentence)\n        positive_score += sentiment['pos']\n        negative_score += sentiment['neg']\n        neutral_score += sentiment['neu']\n        total_sentences += 1\n# normalize scores by dividing polarity score by number of sentences\n    if total_sentences > 0:\n        avg_positive_score = positive_score / total_sentences\n        avg_negative_score = negative_score / total_sentences\n        avg_neutral_score = neutral_score / total_sentences\n# determine aggregate sentiment\n        if avg_positive_score > avg_negative_score:\n            overall_sentiment = \"Positive\"\n        elif avg_positive_score < avg_negative_score:\n            overall_sentiment = \"Negative\"\n        else:\n            overall_sentiment = \"Neutral\"\n\n        return {\n            \"Total Sentences Analyzed\": total_sentences,\n            \"Average Positive Score\": avg_positive_score,\n            \"Average Negative Score\": avg_negative_score,\n            \"Average Neutral Score\": avg_neutral_score,\n            \"Overall Sentiment\": overall_sentiment,\n        }\n    else:\n        return {\"No sentences with 're-' words found for analysis.\"}\n\n# Specify the directories you want to process with aliases\ncorpus_directories = {\n \n        '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n       # Add more directories here\n}\n\n# Process each directory; start by creating an empty list to contain sentences for each directory / corpus\nfor corpus_directory, alias in corpus_directories.items():\n    sentences = []\n\n    # Read in the text files in the corpus and tokenize them into sentences\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n                sentences += nltk.sent_tokenize(text)\n\n    # Filter sentences with words starting with \"re-\"\n    sentences_with_re = [sentence for sentence in sentences if any(word.lower().startswith(\"re-\") for word in nltk.word_tokenize(sentence))]\n\n    # Calculate aggregate sentiment for the filtered sentences\n    results = calculate_aggregate_sentiment(sentences_with_re)\n\n    # Print results for the current directory\n    print(f\"{alias}\")\n    for key, value in results.items():\n        print(f\"{key}: {value}\")\n    print()\n```\n\n## Conclusion {.smaller}\n\n- Text mining morphemes can:\n    - illuminate significant micro-elements of poetic style\n    - show how seeminly trivial features contribute to poetic meaning\n- Distant Reading :handshake: Close Reading  \n\n<br />\n\n\n\n- **Thank You!**  \n- Contact Info\n    - Adam Mazel\n    - Digital Publishing Librarian\n    - Indiana University Bloomington\n    - [amazel@iu.edu](mailto:amazel@iu.edu)","srcMarkdownNoYaml":"\n\n## Why \"Re-\" in VP? {.smaller visibility=\"hidden\"}\n\n:::: {.columns}\n\n::: {.column}\n- NAVSA Conference 2023: \"**<u>Re</u>**vision, **<u>Re</u>**turn, **<u>Re</u>**form\"\n    - Panel: \"Re: Re: Victorian Poetry\"\n        - Adela Pinch, Professor of English, University of Michigan\n        - Emily Harrington, Assoc. Prof. of English, UC Boulder\n        - Naomi Levine, Asst. Prof. of English, Yale University\n        - Me!\n:::\n\n::: {.column}\n![](images/navsa.png){fig-alt=\"2023 NAVSA Conference Banner\" width=\"85%\"}\n:::\n\n::: aside\nNAVSA 2023 Conference banner excerpted from [conference website](https://www.navsa2023.com/)\n:::\n\n::::\n\n## Methodology \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- \"Re-\" :handshake: Text Mining \t\n- Python\n    - Keyword Frequency\n    - Key Word in Context (KWIC)\n    - Term Frequency over Time\n    - Sentiment Analysis\n- Exploratory Data Analysis\n:::\n\n::: {.column width=\"40%\"}\n![](images/python.png){fig-alt=\"python programming language icon\"}\n![](images/plot.png){fig-alt=\"python scatterplot of recipe ingredient frequency before / after 1900\"}\n:::\n\n::: aside\nPython logo used with [permission](https://www.python.org/community/logos/)\n:::\n\n::::\n\n## Data: Dante Gabriel Rossetti\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- *Poems* (1870)\n- *Poems: A New Edition* (1881)\n- *Ballads and Sonnets* (1881)\n:::\n\n::: {.column width=\"40%\"}\n![Self Portrait, 1861](images/dgrossetti.png){fig-alt=\"self-portrait of DG Rossetti\"}\n:::\n\n::: aside\nImage from the [Rossetti Archive](http://www.rossettiarchive.org/docs/s438.rap.html) in public domain in US\n:::\n\n::::\n\n## Data: Algernon Charles Swinburne {.smaller}\n\n:::: {.columns}\n\n::: {.column} \n- *Atalanta in Calydon* (1865)\n- *Poems and Ballads* (1866)\n- *Songs Before Sunrise* (1871)\n- *Songs of Two Nations* (1875)\n- *Erechtheus* (1876)\n- *Poems and Ballads, Second Series* (1878)\n- *Songs of the Springtides* (1880)\n- *Studies in Song* (1880)\n:::\n\n::: {.column}\n- *The Heptalogia, or the Seven against Sense. A Cap with Seven Bells* (1880)\n- *Tristram of Lyonesse* (1882)\n- *A Century of Roundels* (1883)\n- *A Midsummer Holiday and Other Poems* (1884)\n- *Poems and Ballads, Third Series* (1889)\n- *Astrophel and Other Poems* (1894)\n- *The Tale of Balen* (1896)\n- *A Channel Passage and Other Poems* (1904)\n:::\n\n::::\n\n## Data: Michael Field {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- *Long Ago* (1889)\n- *Sight and Song* (1892)\n- *Underneath the Bough* (1893)\n- *Wild Honey from Various Thyme* (1908)\n- *Poems of Adoration* (1912)\n- *Mystic Trees* (1913)\n- *Whym Chow: Flame of Love* (1914)\n:::\n\n::: {.column width=\"40%\"}\n![Katherine Bradley & Edith Cooper, aka Michael Field](images/field.jpg){fig-alt=\"photo of Katherine Bradley & Edith Cooper\"}\n:::\n\n::: aside\nImage from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Katherine_Harris_Bradley_%26_Edith_Emma_Cooper_(2).jpg) in public domain in US\n:::\n\n::::\n\n## Data: Thomas Hardy {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- *Wessex Poems and Other Verses* (1898)\n- *Poems of the Past and the Present* (1901)\n- *Time's Laughingstocks and Other Verses* (1909)\n- *Satires of Circumstance* (1914)\n- *Moments of Vision* (1917)\n- *Late Lyrics and Earlier with Many Other Verses* (1922)\n- *Human Shows, Far Phantasies, Songs and Trifles* (1925)\n:::\n\n::: {.column width=\"40%\"}\n![](images/hardy.jpg){fig-alt=\"headshot of Thomas Hardy\" width=\"63%\"}\n:::\n\n::: aside\nImage from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Thomashardy_restored.jpg) in public domain in US\n:::\n\n::::\n\n## Who Uses \"Re-\" Words More / Less?\n\n- Keyword Frequency Analysis\n    - What percent of each poet’s corpus is composed of words with the prefix \"re-\"?\n    - Compare \"Re-\" Word Percentages\n\n```{python}\n# import software libraries\nimport nltk\nimport os\nimport matplotlib.pyplot as plt\n\n# Download NLTK tokenizer data\nnltk.download('punkt') \n\n# Define a function that tokenizes a text and then counts how many of its words start with \"re-\"\ndef count_re_words(text):\n    words = nltk.word_tokenize(text)\n    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n\n# Specify the directory paths for the two poets' corpora\ncorpus_directories = {\n    'swinburne': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n    'hardy': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n    'michael field': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n    'dg rossetti': '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n}\n\n# Initialize dictionary to store the results\npercentage_re_words = {}\n\n# create an empty list for each poet/directory\nfor poet, corpus_directory in corpus_directories.items():\n    corpus = []\n    \n    # Read the text files in the poet's corpus and add them to the apt list above\n    for filename in os.listdir(corpus_directory):\n        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n            text = file.read()\n            corpus.append(text)\n\n    # Apply fuction above to tokenize + count the \"re-\" words in each text of the poet's corpus and store count\n    re_word_count = sum(count_re_words(text) for text in corpus)\n\n    # Calculate the corpus word count by taking each text, tokenizing it, counting the number of tokens, and then adding them to create a total word count \n    total_words = sum(len(nltk.word_tokenize(text)) for text in corpus)\n    # Calculate the percentage of \"re-\" words in the poet's corpus by dividing the re- word count by the total word count and multiply by 100\n    percentage_re_words[poet] = (re_word_count / total_words) * 100\n\n# Sort the results from largest to smallest\nsorted_results = sorted(percentage_re_words.items(), key=lambda x: x[1], reverse=True)\n\n# Extract poets and percentages for plotting\npoets, percentages = zip(*sorted_results)\n\n# Step 4: visualize the results in a bar chart \nplt.figure(figsize=(8, 6))\nplt.bar(poets, percentages, color=['blue', 'orange'])\nplt.ylabel('Percentage (%)')\nplt.title('Whose Poetry is More Composed of Words that Start with \"Re-\"?')\n\n# Set the y-axis limit based on the largest percentage\nylim_percentage = max(percentages) * 2  # Adjusted for better visualization\nplt.ylim(0, ylim_percentage)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Display the bar chart with the poet's names angled to avoid overlap\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n## Which \"Re-\" Words Are Most Frequent?\n\n- Keyword Frequency Analysis\n    - Which \"re-\" words are used and how often?\n\n```{python}\n# import software libraries / dependencies\nimport nltk\nimport os\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom nltk.stem import SnowballStemmer\n\n# Download NLTK tokenizer data\nnltk.download('punkt')  \n\n# Initialize Stemmer for English\nstemmer = SnowballStemmer(\"english\")  \n\n# create function to preprocess and process files of each directory: create an empty list for each directory/poet\ndef process_directory(corpus_directory):   \n    corpus = []\n\n    # Get the directory (poet's) name to use as the label\n    label = os.path.basename(corpus_directory)\n\n    # Step 1: Read the text of corpus from files and add it to apt (poet's) list, created above\n    for filename in os.listdir(corpus_directory):\n        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n            text = file.read()\n            corpus.append(text)\n\n    # Step 2: Tokenize each text into individual words\n    tokenized_corpus = [nltk.word_tokenize(text) for text in corpus]\n\n    # Step 3: find words that when lower cased start with re-, standardize (lower) their case, stem them, retain them\n    stemmed_corpus = []\n    for tokens in tokenized_corpus:\n        stemmed_tokens = [stemmer.stem(word.lower()) for word in tokens if re.match(r'\\b(re-)\\w+', word.lower())]\n        stemmed_corpus.append(stemmed_tokens)\n\n    # Step 6: Count the frequency of each re- word\n    word_counts = Counter(word for tokens in stemmed_corpus for word in tokens)\n\n    # Step 7: Display the most frequent words\n    # most_common_re_words = word_counts.most_common(20)  # Set the desired number of top words\n    # for word, count in most_common_re_words:\n    #    print(f'The poetry of {label[:-5]} uses {word}: {count} times')\n\n    # Step 8: Plot top 20 most frequent words and counts in a bar chart, use lable to title plot\n    plt.figure(figsize=(10, 5))\n    top_words, top_counts = zip(*word_counts.most_common(20))\n    plt.bar(top_words, top_counts)\n    plt.title(f'Frequency of Stemmed Re- Words in {label[:-5].capitalize()}\\'s Poetry')\n    plt.xticks(rotation=65)\n    plt.show()\n\n# Directories to process\ncorpus_directories = [\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n    # Add more directories here\n]\n\n# Process each directory\nfor directory in corpus_directories:\n    process_directory(directory)\n```\n\n## Field's and Hardy's \"Re-Illume\" in Context {.smaller}\n\n- Re-illume\n    - Extremely rare\n    - Chiefly poetic\n    - 1758 &ndash; present\n\n- Field: 1x\n    - [\"XXII: Sleeping together: Sleep\"](https://editions.covecollective.org/edition/whym-chow-flame-love/whym-chow-flame-love#chapter22): *Whym Chow: Flame of Love* (1906, 1914)\n- Hardy: 2x\n    - \"Two Rosalinds\": *Time's Laughingstocks and Other Verses* (1909)\n    - \"For Life I had never cared greatly\": *Moments of Vision and Miscellaneous Verses* (1917)\n\n## {background-iframe=\"https://books.google.com/ngrams/graph?content=reillumed%2Breillume%2Breilluming%2Breillumes&year_start=1779&year_end=2019&corpus=en-GB-2019&smoothing=3\" background-interactive=\"true\"}\n\n## Do \"Re-\" Words and \"Re\" Words Co-occur in Hardy's Poetry?\n\n- Key (Re-) Word in Context (KWIC)\n- Have Python return Hardy's sentences that contain word(s) that start with \"re-\" and word(s) that start with \"re\"\n\n```{python}\n# import libraries\nimport os # open directories on local machine\nimport nltk # nlp\nimport re # reg ex\nfrom nltk.tokenize import sent_tokenize, word_tokenize # break text blob into individual sentences and words\n\n# Define regular expressions\nre_pattern = r'\\b(?:[Rr]e|[Rr]E)\\w+\\b'  # Matches \"re\" or \"Re\" or \"rE\" or \"RE\" followed by one or more letters (not hyphen)\nre_hyphen_pattern = r'\\b(?:[Rr]e-|[Rr]E-)\\w+\\b'  # Matches \"re-\" or \"Re-\" or \"rE-\" or \"RE-\" followed by one or more letters\n# \\b matches a word boundary.\n# (?:[Rr]e|[Rr]E) is a non-capturing group that matches either \"re\" or \"Re\" or \"rE\" or \"RE\" \n# \\w+ matches one or more word characters following \"re\" or \"Re\" or \"rE\" or \"RE\" \n\ndef matches_pattern(word, pattern):\n    return re.search(pattern, word, re.IGNORECASE) is not None\n\n# Define function to surround matched words with asterisks\ndef bold_matched_words(match, pattern):\n    return f'**{match.group()}**'\n\n# Directory containing text files\ntext_directory = \"/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP\"\n\n# Iterate through text files in the directory, open text files, read in text, break up text into sentences, break up sentences into words, id those sentences with re/re- words, if sentence has re/re-, add asterisks around re-/re words and print sentence\nfor filename in os.listdir(text_directory):\n    if filename.endswith(\".txt\"):\n        with open(os.path.join(text_directory, filename), \"r\", encoding=\"utf-8\") as file:\n            text = file.read()\n            sentences = sent_tokenize(text)\n            \n            for sentence in sentences:\n                words = word_tokenize(sentence)\n                has_re = any(matches_pattern(word, re_pattern) for word in words)\n                has_re_hyphen = any(matches_pattern(word, re_hyphen_pattern) for word in words)\n                \n                if has_re and has_re_hyphen:\n                    sentence_with_bold = re.sub(re_pattern, lambda x: bold_matched_words(x, re_pattern), sentence)\n                    sentence_with_bold = re.sub(re_hyphen_pattern, lambda x: bold_matched_words(x, re_hyphen_pattern), sentence_with_bold)\n                    print(sentence_with_bold)\n                    print(\"\\n\" * 2)\n```\n\n## When Are \"Re-\" Words More / Less Frequent?\n\n- Time Series / Term Frequency over Time\n    - Y axis: re- word percentage of book\n    - X axis: publication year of book\n\n```{python}\n# Import software libraries\nimport nltk # nlp\nimport os # interact with directories on local machine\nimport re # reg ex\nimport matplotlib.pyplot as plt # data visualization\nimport string # punctuation removal\n\n# Download NLTK tokenizer data\nnltk.download('punkt')  \n\n# Define a function that takes each text, tokenizes it into individual words, and count words that when lowercased start with \"re-\"\ndef count_re_words(text):\n    words = nltk.word_tokenize(text)\n    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n\n# Define a function to remove all punctuation except hyphens\ndef remove_punctuation_except_hyphens(text):\n    translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n    return text.translate(translator)\n\n# Specify the parent directory containing multiple text file directories\nparent_directory = '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/'\n\n# Soecific irectories to process\nall_directory_names = [\n    'swinburne/swinburne_noBP',\n    'hardy/hardy_noBP',\n    'field/field_NoBP',\n    # 'rossetti_dg/rossetti_dg_NoBP',\n]\n\n# Initialize a dictionary to store year-wise percentages\nyear_percentages = {}\n\n# Process each directory\nfor directory_name in all_directory_names:\n    # Construct the full path to the current directory\n    text_files_directory = os.path.join(parent_directory, directory_name)\n\n    # Initialize two dictionaries: one to store year-wise percentages for the current directory and one to store file names for the current directory\n    if os.path.isdir(text_files_directory):\n        directory_percentages = {}\n        file_names = {}\n\n        # Process each text file in the current directory\n        for filename in os.listdir(text_files_directory):\n            if filename.endswith('.txt'):\n                # Extract the year from the filename using a regular expression\n                year_match = re.match(r'(\\d{4})_', filename)\n                if year_match:\n                    year = int(year_match.group(1))\n                    # read in text of each book of poetry\n                    with open(os.path.join(text_files_directory, filename), 'r', encoding='utf-8') as file:\n                        text = file.read()\n                        # Remove punctuation except hyphens\n                        text = remove_punctuation_except_hyphens(text)\n                        # tokenize text, count the total number of words, count the number of re- words, normalize counts\n                        total_words = len(nltk.word_tokenize(text))\n                        re_word_count = count_re_words(text)\n                        percentage_re_words = (re_word_count / total_words) * 100\n                        \n                        #add to dictionary: value: count, key: publication year\n                        directory_percentages[year] = percentage_re_words\n                        \n                        # Extract the text between underscores in the filename\n                        file_name_parts = filename.split('_')\n                        if len(file_name_parts) > 2:\n                            file_name = '_'.join(file_name_parts[1:-1])\n                        else:\n                            file_name = file_name_parts[1]\n                        \n                        #add to dictionary: value: filename, key: years\n                        file_names[year] = file_name  # Store the extracted file name\n\n        # Sort the dictionary by keys (years) for the current directory\n        sorted_directory_percentages = {year: directory_percentages[year] for year in sorted(directory_percentages)}\n\n        # Store the results for the current directory\n        year_percentages[directory_name] = {\n            'percentages': sorted_directory_percentages,\n            'file_names': file_names  # Store file names for this directory\n        }\n\n# Step 2: Plot the keys (years) and values (percentages) in a line graph for each directory\nplt.figure(figsize=(10, 8))\n\nfor directory_name, data in year_percentages.items():\n    percentages = data['percentages']\n    file_names = data['file_names']\n    years = list(percentages.keys())\n    percentages = list(percentages.values())\n    \n    # Plot the data points\n    plt.plot(years, percentages, marker='o', linestyle='-', label=directory_name[:-5])\n    \n    # Add annotations for each data point if the value is greater than 0\n    for year, percentage in zip(years, percentages):\n        if percentage > 0:\n            file_name = file_names[year]\n            annotation = f\"{file_name}\"\n            plt.annotate(annotation, (year, percentage), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n\nplt.ylim(0, 0.0525)\nplt.xlabel('Year')\nplt.ylabel('Percentage of Words Starting with \"Re-\"')\nplt.title('When Are \"Re-\" Words Used in Field\\'s, Hardy\\'s, and Swinburne\\'s Poetry?')\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.legend()  # Show legend indicating directory names\n\n# Display the line graph\nplt.tight_layout()\nplt.show()\n```\n\n## Are \"Re-\" Words Used Positively or Negatively?\n\n- Sentiment Analysis\n    - determines a text's emotional tone (positive / negative / neutral)\n\n```{python}\n# import software libraries\nimport nltk #nlp\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer # VADER sentiment analyzer\nimport os # enable engagement with directories and files on local machine\n\n# Initialize the VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Function to calculate aggregate sentiment (positive / neutral / negative) for a collection of sentences\n# start counts at zero\ndef calculate_aggregate_sentiment(sentences):\n    positive_score = 0\n    negative_score = 0\n    neutral_score = 0\n    total_sentences = 0\n\n# for each sentence, calculate sentiment polarity score: if score is positive, add positive score to positive score count; lastly keep a running count of the number of sentences analyzed\n    for sentence in sentences:\n        sentiment = sia.polarity_scores(sentence)\n        positive_score += sentiment['pos']\n        negative_score += sentiment['neg']\n        neutral_score += sentiment['neu']\n        total_sentences += 1\n# normalize scores by dividing polarity score by number of sentences\n    if total_sentences > 0:\n        avg_positive_score = positive_score / total_sentences\n        avg_negative_score = negative_score / total_sentences\n        avg_neutral_score = neutral_score / total_sentences\n# determine aggregate sentiment\n        if avg_positive_score > avg_negative_score:\n            overall_sentiment = \"Positive\"\n        elif avg_positive_score < avg_negative_score:\n            overall_sentiment = \"Negative\"\n        else:\n            overall_sentiment = \"Neutral\"\n\n        return {\n            \"Total Sentences Analyzed\": total_sentences,\n            \"Average Positive Score\": avg_positive_score,\n            \"Average Negative Score\": avg_negative_score,\n            \"Average Neutral Score\": avg_neutral_score,\n            \"Overall Sentiment\": overall_sentiment,\n        }\n    else:\n        return {\"No sentences with 're-' words found for analysis.\"}\n\n# Specify the directories you want to process with aliases\ncorpus_directories = {\n \n        '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n       # Add more directories here\n}\n\n# Process each directory; start by creating an empty list to contain sentences for each directory / corpus\nfor corpus_directory, alias in corpus_directories.items():\n    sentences = []\n\n    # Read in the text files in the corpus and tokenize them into sentences\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n                sentences += nltk.sent_tokenize(text)\n\n    # Filter sentences with words starting with \"re-\"\n    sentences_with_re = [sentence for sentence in sentences if any(word.lower().startswith(\"re-\") for word in nltk.word_tokenize(sentence))]\n\n    # Calculate aggregate sentiment for the filtered sentences\n    results = calculate_aggregate_sentiment(sentences_with_re)\n\n    # Print results for the current directory\n    print(f\"{alias}\")\n    for key, value in results.items():\n        print(f\"{key}: {value}\")\n    print()\n```\n\n## Conclusion {.smaller}\n\n- Text mining morphemes can:\n    - illuminate significant micro-elements of poetic style\n    - show how seeminly trivial features contribute to poetic meaning\n- Distant Reading :handshake: Close Reading  \n\n<br />\n\n\n\n- **Thank You!**  \n- Contact Info\n    - Adam Mazel\n    - Digital Publishing Librarian\n    - Indiana University Bloomington\n    - [amazel@iu.edu](mailto:amazel@iu.edu)"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":true,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","from":"markdown+emoji","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en-US","fig-responsive":true,"quarto-version":"1.3.450","auto-stretch":true,"title":"Text Mining \"Re-\" in Victorian Poetry","date":"11 November 2023","author":"Adam Mazel, Digital Publishing Librarian","institute":"Scholarly Communication Department, IUB Libraries","preloadIframes":true,"output-location":"slide","scrollable":true,"chalkboard":true,"overview":true,"touch":true,"progress":true,"slideNumber":true,"previewLinks":true,"transition":"slide","keywords":["Digital Humanities","Text Mining","Victorian Poetry","re"]}}},"projectFormats":["revealjs"]}
{"title":"Thirteen* Digital Ways of Looking at -re- in Victorian Poetry","markdown":{"headingText":"Rationale / Significance","containsRefs":false,"markdown":"\n- Analyzing \"Re-\" is apt for text mining\n    - small details: hard to notice with one's eyes, but easy to notice with computer \"vision\"\n    - play with focus of analytic lens\n        - (too-)close reading: microscopic aspects of language\n        - distant reading: macroscopic methods of analysis\n\n## Method\n\n- Exploratory Data Analysis (EDA)\n    - early research: explore data to discover trends and generate hypotheses / basic insights\n- Python (Natural Language ToolKit (NLTK), Matplotlib)\n    - (simple) Count-based methods, rather than (complex) machine-  / deep-learning\n        - Apt for EDA / early research, feature (\"re-\") analysis, smaller datasets\n\n## Data: Which Authors / Texts + Why?\n\n- DG Rossetti\n    - *Poems* (1870)\n    - *Poems: A New Edition* (1881)\n    - *Ballads and Sonnets* (1881)\n- AC Swinburne\n    - *Atalanta in Calydon* (1865)\n    - *Poems and Ballads* (1866)\n    - *Songs Before Sunrise* (1871)\n    - *Songs of Two Nations* (1875)\n    - *Erechtheus* (1876)\n    - *Poems and Ballads, Second Series* (1878)\n    - *Songs of the Springtides* (1880)\n    - *Studies in Song* (1880)\n    - *The Heptalogia, or the Seven against Sense. A Cap with Seven Bells* (1880)\n    - *Tristram of Lyonesse* (1882)\n    - *A Century of Roundels* (1883)\n    - *A Midsummer Holiday and Other Poems* (1884)\n    - *Poems and Ballads, Third Series* (1889)\n    - *Astrophel and Other Poems* (1894)\n    - *The Tale of Balen* (1896)\n    - *A Channel Passage and Other Poems* (1904)\n- Michael Field\n    - *Long Ago* (1889)\n    - *Sight and Song* (1892)\n    - *Underneath the Bough* (1893)\n    - *Wild Honey from Various Thyme* (1908)\n    - *Poems of Adoration* (1912)\n    - *Mystic Trees* (1913)\n    - *Whym Chow: Flame of Love* (1914)\n- Thomas Hardy\n    - *Wessex Poems and Other Verses* (1898)\n    - *Poems of the Past and the Present* (1901)\n    - *Time's Laughingstocks and Other Verses* (1909)\n    - *Satires of Circumstance* (1914)\n    - *Moments of Vision* (1917)\n    - *Late Lyrics and Earlier with Many Other Verses* (1922)\n    - *Human Shows, Far Phantasies, Songs and Trifles* (1925)\n\n- Where Acquired\n    - [Project Gutenberg](https://www.gutenberg.org/)\n    - [HathiTrust](https://www.hathitrust.org/)\n    - [COVE (Collaborative Organization for Virtual Education)](https://editions.covecollective.org/)\n\n- Data Cleaning\n    - Removed noise: Boilerplate, Title Pages, Tables of Contents, Advertisments, Endorsements, Headers + Foorters (most), Unusual Characters\n- Data Quality \n    - OCR: Errors\n    - Noise (headers + footers, etc.)\n\n## Which \"Re-\" Words Are Most Frequent: Theory\n\n#### How\n- Keyword Frequency\n    - ID which re- words are used and count how often\n    - Fundamental method of text mining (TM)\n\n#### Why\n- Uncover meaningful patterns in language use\n    - Significant terms\n    - Style\n    - Aboutness: Themes / Topics\n- TM starting point\n- Apt for analyses of specific details / features\n\n## Which \"Re-\" Words Are Most Frequent: Code\n- Have computer count all words that start with \"re-\" in poet's corpus + visualize results in bar chart\n- Stemming\n    - Remove inflected endings to condense different versions of same term to a common stem\n    - \"re-enter: 1\", \"re-entering: 1\", \"re-entered: 1\" --> \"re-ent: 3\"\n    - Improves IDing significant concepts\n\n```{python}\n#| echo: true\n#| output-location: slide\n \n# import software libraries / dependencies\nimport nltk\nimport os\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('punkt')  # Download NLTK tokenizer data\n\nstemmer = SnowballStemmer(\"english\")  # Initialize SnowballStemmer for English\n\n# create function to preprocess and process files of each directory\ndef process_directory(corpus_directory):   \n    corpus = []\n\n    # Get the directory name as the label\n    label = os.path.basename(corpus_directory)\n\n    # Step 1: Get the text of corpus from files\n    for filename in os.listdir(corpus_directory):\n        with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n            text = file.read()\n            corpus.append(text)\n\n    # Step 2: Tokenize the text blob into individual words\n    tokenized_corpus = [nltk.word_tokenize(text) for text in corpus]\n\n    # Step 3: Standardize (lower) case, find words that start with re-, stem those words, retain them\n    stemmed_corpus = []\n    for tokens in tokenized_corpus:\n        stemmed_tokens = [stemmer.stem(word.lower()) for word in tokens if re.match(r'\\b(re-)\\w+', word.lower())]\n        stemmed_corpus.append(stemmed_tokens)\n\n    # Step 6: Count the frequency of each re- word\n    word_counts = Counter(word for tokens in stemmed_corpus for word in tokens)\n\n    # Step 7: Display the most frequent words\n    # most_common_re_words = word_counts.most_common(20)  # Set the desired number of top words\n    # for word, count in most_common_re_words:\n    #    print(f'The poetry of {label[:-5]} uses {word}: {count} times')\n\n    # Step 8: Plot words and counts in a bar chart\n    plt.figure(figsize=(10, 5))\n    top_words, top_counts = zip(*word_counts.most_common(20))\n    plt.bar(top_words, top_counts)\n    plt.title(f'Frequency of Stemmed Re- Words in {label[:-5].capitalize()}\\'s Poetry')\n    plt.xticks(rotation=65)\n    plt.show()\n\n# Directories to process\ncorpus_directories = [\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP',\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP',\n    # Add more directories here\n]\n\n# Process each directory\nfor directory in corpus_directories:\n    process_directory(directory)\n```\n\n## Results: Take Aways\n- Keywords\n    - Swinburne: \"Re-Risen: 12\" (80% of S's \"Re-\" Words)\n        - \"re-risen\": content: interested in ressurection?\n        - \"re-risen: form: alliterative (more echoic than \"ressurect\")\n    - Hardy: \"Re-Enact: 4\" (20% of H's \"Re-\" Words)\n- Comparison of \"Re-\" Use\n\n|                    | Low Vocabulary | High Vocabulary |\n|------------------- | ---------------| ----------------|\n| **High Frequency** | Swinburne      |                 |\n| **Low Frequency**  | Rossetti       | Hardy, Field    |\n\n- Rossetti's \"Re-Born\"\n    - Hapax Legomenon\n        - [\"Ardour & Memory\"](http://www.rossettiarchive.org/docs/4-1873.tinkerms.rad.html) (1879), Sonnet LXIV, *House of Life* __Ballads and Sonnets__ (1881)\n            - \"The furtive flickering streams to light re-born / 'Mid airs new-fledged & valorous lusts of morn,\"\n        - But hyphenated prefixes (e.g. \"a-heap\", \"to-night\") and compound words (e.g. \"cukoo-throb\", \"forest-boughs\") are common (1.22%) in DGR\n        - \"ressurect*\" appears only once, \"born: 48\" and \"birth: 46\" \n        - Hypotheses: uninterested in signifying repetition through \"re-\", more interested in birth than rebirth?\n\n- Hardy and Field: \"Re-Illume\"\n    - Chiefly poetic\n    - Extremely rare (OED Band 1)\n    - 1758 - present\n    - Hardy (2)\n        - \"Two Rosalinds\": __Time's Laughingstocks and Other Verses__ (1909)\n            - \"But the well-known numbers needed not for me a text or teacher / To **revive** and **re-illume.**\"\n            - \"re-\" relatively frequent in poem; poem about nostalgia for an actress\n        - \"For Life I had never cared greatly\": __Moments of Vision and Miscellaneous Verses__ (1917)\n            - Concludes with \"re\" words: \"I pace hill and dale / **Regarding** the sky, / **Regarding** the vision on high, / And thus **re-illumed** have no humour for letting / My pilgrimage fail.\"\n        - \"re-\" words often juxtaposed with \"re\" words (\"The Flirt's Tragedy\" etc.)\n    - Field (1)\n        - [\"XXII: Sleeping together: Sleep\"](https://editions.covecollective.org/edition/whym-chow-flame-love/whym-chow-flame-love#chapter22) __Whym Chow: Flame of Love__ (1914)\n            - \"We with all our deeds and dreams / Re-illumed, as were the gleams / Of thy savouring body\"\n            - \"re-\" relatively frequent in book\n    - rare word declining in use and becoming obsolete: perpetuate, revive and re-illume a declining word\n\n## {background-iframe=\"https://books.google.com/ngrams/interactive_chart?content=reillume_INF,(reillumed+++reillume+++reilluming+++reillumes)&year_start=1800&year_end=2019&corpus=en-GB-2019&smoothing=3\" background-interactive=\"true\"}\n\n## Which Words Adjoin \"Re-\" Words?: Theory\n\n#### How\n\n- Bigrams (n-grams)\n    - Bigrams: two consecutive words (Trigram: three consecutive words) \n        - e.g. \"She used the olive oil.\": \"She used\", \"used the\", \"the olive\", \"olive oil\"\n    - ID / count bigrams of re- words: re- word + consecutive word\n\n#### Why\n\n    - Key Word in Context: Know a word by the company it keeps: facilitaate understadning of key word\n        - e.g. \"olive\", \"oil\"--> \"olive oil\"\n    - Better understand re- words by learning their immediate contexts and associations via frequently co-occuring terms\n    - Fundamental method of text mining (TM)\n\n## Which Words Adjoin \"Re-\" Words?: Code\n\n- Have computer ID all the bigrams of each poet's works and then filter everything but bigrams of words that start with re- \n- remove stopwords (meaningless function words) to reveal associated concepts\n\n```{python}\n#| echo: true\n#| output-location: slide\n\n# import software dependencies / libraries\nimport nltk\nimport os\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom collections import Counter\nimport string\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('stopwords')\n\n# Directories to process, Aliases \ncorpus_directories = {\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n}\n\n# RegEx to match words starting with \"re-\"\nre_word_pattern = r'\\b(re-\\w+)'\n\n# Dictionary to store directory/poet: bigram frequencies \nbigram_frequencies = {alias: Counter() for alias in corpus_directories.values()}\n\n# Define the punctuation to remove (including curly quotation marks)\npunctuation_to_remove = string.punctuation.replace('-', '') + \"‘’“”\"\n\n# Preprocess and Process each directory\nfor corpus_directory, alias in corpus_directories.items():\n    # Get the text from the text files in the corpus\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n\n                # Tokenize the text\n                tokens = nltk.word_tokenize(text)\n\n                # Remove punctuation except hyphens and standardize case\n                translator = str.maketrans('', '', punctuation_to_remove)\n                preprocessed_text = ' '.join(tokens).translate(translator).lower()\n\n                # Remove stopwords\n                stop_words = set(stopwords.words(\"english\"))\n                filtered_tokens = [word for word in preprocessed_text.split() if word not in stop_words]\n\n                # Find bigrams\n                bigrams = list(ngrams(filtered_tokens, 2))\n\n                # Filter bigrams to include only those starting with \"re-\"\n                re_bigrams = [bigram for bigram in bigrams if re.match(re_word_pattern, bigram[0])]\n\n                # Count the bigrams\n                bigram_frequency = Counter(re_bigrams)\n\n                # Add the bigram frequencies as values of the current directory/poet as keys\n                bigram_frequencies[alias].update(bigram_frequency)\n\n# Print the sorted bigram frequencies per directory/poet\nfor alias, frequencies in bigram_frequencies.items():\n    print(f\"{alias}:\")\n    for bigram, frequency in sorted(frequencies.items(), key=lambda x: x[0]):  # Sort alphabetically\n        print(f\"{bigram}: {frequency}\")\n    print()\n```\n\n## Results: Take Aways\n\n- Swinburne\n    - \"re-__\" : \"life\", \"dead\" (2x), \"dust\", \"mortal\"\n        - re- associated with life + death, mortality, etc.\n    - \"re-risen\": \"prison\"\n        - re-risen as internal rhyme: \"re\" \"ri\" \"pri\"\n- Hardy\n    - \"re-____\": \"time\" (2x), \"semipiternal\" (everlasting), \"killing\", \"death\", \"lives\" \"new\", \"olden\"\n        - re- associated with time, eternal / mortal, etc.\n- Field\n    - \"re-appear\": \"fade\", \"re-light\": \"tarnished\"\n        - re- used with opposites\n\n## When Were \"Re-\" Words More / Less Frequent: Theory\n\n#### How\n\n- Time Series / Term Frequency over Time\n- ID when (in poet's career) \"Re-\" words were used more / less often\n\n### Why\n\n- When in poet's career did \"re-\" become more / less frequent\n    - Why did \"re-\" wax / wane then?\n- In which book(s) is re- frequent / infrequent? Why?\n\n## When Were \"Re-\" Words More / Less Frequent: Code\n\n- Computer counts what percent each book of poetry is \"re-\" words (total number of re- words divided by total number of words)\n    - nomalizing the data enables comparison\n- Y axis: re- word percentage of book; X axis: publication year of book\n\n```{python}\n#| echo: true\n#| output-location: slide\n\n# Import software libraries / dependencies\nimport nltk\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport string\n\nnltk.download('punkt')  # Download NLTK tokenizer data\n\n# Define a function to count words that start with \"re-\"\ndef count_re_words(text):\n    words = nltk.word_tokenize(text)\n    return sum(1 for word in words if word.lower().startswith(\"re-\"))\n\n# Define a function to remove all punctuation except hyphens\ndef remove_punctuation_except_hyphens(text):\n    translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n    return text.translate(translator)\n\n# Specify the parent directory containing multiple text file directories\nparent_directory = '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/'\n\n# Soecific irectories to process\nall_directory_names = [\n    'swinburne/swinburne_noBP',\n    'hardy/hardy_noBP',\n    'field/field_NoBP',\n    'rossetti_dg/rossetti_dg_NoBP',\n]\n\n# Initialize a dictionary to store year-wise percentages\nyear_percentages = {}\n\n# Process each directory\nfor directory_name in all_directory_names:\n    # Construct the full path to the current directory\n    text_files_directory = os.path.join(parent_directory, directory_name)\n\n    if os.path.isdir(text_files_directory):\n        # Initialize a dictionary to store year-wise percentages for the current directory\n        directory_percentages = {}\n\n        # Initialize a dictionary to store file names for the current directory\n        file_names = {}\n\n        # Process each text file in the current directory\n        for filename in os.listdir(text_files_directory):\n            if filename.endswith('.txt'):\n                # Extract the year from the filename using a regular expression\n                year_match = re.match(r'(\\d{4})_', filename)\n                if year_match:\n                    year = int(year_match.group(1))\n                    with open(os.path.join(text_files_directory, filename), 'r', encoding='utf-8') as file:\n                        text = file.read()\n                        # Remove punctuation except hyphens\n                        text = remove_punctuation_except_hyphens(text)\n                        # normalize counts\n                        total_words = len(nltk.word_tokenize(text))\n                        re_word_count = count_re_words(text)\n                        percentage_re_words = (re_word_count / total_words) * 100\n                        \n                        #add to dictionary: values: counts to keys: years\n                        directory_percentages[year] = percentage_re_words\n                        \n                        # Extract the text between underscores in the filename\n                        file_name_parts = filename.split('_')\n                        if len(file_name_parts) > 2:\n                            file_name = '_'.join(file_name_parts[1:-1])\n                        else:\n                            file_name = file_name_parts[1]\n                        \n                        #add to dictionary: values: filenames to keys: years \n                        file_names[year] = file_name  # Store the extracted file name\n\n        # Sort the dictionary by keys (years) for the current directory\n        sorted_directory_percentages = {year: directory_percentages[year] for year in sorted(directory_percentages)}\n\n        # Store the results for the current directory\n        year_percentages[directory_name] = {\n            'percentages': sorted_directory_percentages,\n            'file_names': file_names  # Store file names for this directory\n        }\n\n# Step 2: Plot the keys (years) and values (percentages) in a line graph for each directory\nplt.figure(figsize=(15, 8))\n\nfor directory_name, data in year_percentages.items():\n    percentages = data['percentages']\n    file_names = data['file_names']\n    years = list(percentages.keys())\n    percentages = list(percentages.values())\n    \n    # Plot the data points\n    plt.plot(years, percentages, marker='o', linestyle='-', label=directory_name[:-5])\n    \n    # Add annotations for each data point if the value is greater than 0\n    for year, percentage in zip(years, percentages):\n        if percentage > 0:\n            file_name = file_names[year]\n            annotation = f\"{file_name}\"\n            plt.annotate(annotation, (year, percentage), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n\nplt.ylim(0, 0.0525)\nplt.xlabel('Year')\nplt.ylabel('Percentage of Words Starting with \"Re-\"')\nplt.title('When Were \"Re-\" Words Used in Field\\'s, Hardy\\'s, DG Rossetti\\'s, and Swinburne\\'s Poetry?')\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.legend()  # Show legend indicating directory names\n\n# Display the line graph\nplt.tight_layout()\nplt.show()\n```\n\n## Results: Take Aways\n\n- Swinburne\n    - \"Re-\": of interest in mid career, especially in __A Century of Roundels__ (1883)\n        - \"Re-\" as theme of repetition apt for Roundel form: repetitive\n- Field\n    - \"Re-\": of very strong interest in __Whym Chow: Flame of Love__ (1914)\n        - \"Re-\": intimates \"again-ness\", return, and memory: apt for an elegy (dog) \n\n## Are \"Re-\" Words Used Positively or Negatively? Theory\n\n#### How\n\n- Sentiment Analysis\n- determines (part of) a text's emotional tone (positive / negative / neutral)\n\n#### Why\n\n- Opinion mining / reception\n- Characters' sentiments / emotional arcs\n- Sentiment of plot: ID emotional highs / lows\n- Emotional valences / connotations of keywords (\"Re-\" words)\n    - Stylistics: Author's tonal / emotional preferences\n    - Comparison: compare how poets treat emotional associations of \"Re-\" words\n\n## Are \"Re-\" Words Used Positively or Negatively? Code\n\n- Sentiment Analyzer: VADER (Valence Aware Dictionary and sEntiment Reasoner)\n    - lexicon-based approach\n        - VADER employs a lexicon that contains thousands of words and their polarity scores, indicating whether the word is positive, negative, or neutral.\n        - VADER also considers intensifiers (e.g., \"very\" or \"extremely\") that modify the sentiment of adjacent words.\n        - VADER also considers punctuation, such as exclamation marks and question marks, which can influence sentiment.\n        - VADER considers capitalization, giving more weight to fully capitalized words (e.g., \"HAPPY\") and less weight to words in all lowercase.\n    - limitations: sarcasm, irony, or tone\n- Sentiment Scores of Sentences with \"Re-\" Words\n    - aggregate sentiment scores of all sentences with re- words\n- Setiment Scores of each Corpus and\n    - aggregate sentiment scores of entire corpus\n    - serve as a norm against which to compare / contrast Sentiment Scores of Sentences with \"Re-\" Words\n\n```{python}\n#| echo: true\n#| output-location: slide\n\n# Import software libraries / dependencies\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport os\nimport re\nimport matplotlib.pyplot as plt\n\n# Initialize the VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Function to calculate aggregate sentiment for a collection of sentences\ndef calculate_aggregate_sentiment(sentences):\n    positive_score = 0\n    negative_score = 0\n    neutral_score = 0\n    total_sentences = 0\n\n    for sentence in sentences:\n        sentiment = sia.polarity_scores(sentence)\n        positive_score += sentiment['pos']\n        negative_score += sentiment['neg']\n        neutral_score += sentiment['neu']\n        total_sentences += 1\n\n    if total_sentences > 0:\n        avg_positive_score = positive_score / total_sentences\n        avg_negative_score = negative_score / total_sentences\n        avg_neutral_score = neutral_score / total_sentences\n\n        if avg_positive_score > avg_negative_score:\n            overall_sentiment = \"Positive\"\n        elif avg_positive_score < avg_negative_score:\n            overall_sentiment = \"Negative\"\n        else:\n            overall_sentiment = \"Neutral\"\n\n        return {\n            \"Total Sentences Analyzed\": total_sentences,\n            \"Average Positive Score\": avg_positive_score,\n            \"Average Negative Score\": avg_negative_score,\n            \"Average Neutral Score\": avg_neutral_score,\n            \"Overall Sentiment\": overall_sentiment,\n        }\n\n# Specify the directories you want to process with aliases\ncorpus_directories = {\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n    # Add more directories here\n}\n\n# Create lists to store poet names and their corresponding score differences\npoet_names = []\nscore_differences = []\n\n# Process each directory\nfor corpus_directory, alias in corpus_directories.items():\n    sentences = []\n\n    # Read and preprocess the text files in the corpus\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n                sentences += nltk.sent_tokenize(text)\n\n    # Calculate aggregate sentiment\n    results = calculate_aggregate_sentiment(sentences)\n\n    # Calculate the difference between positive and negative scores\n    positive_score = results[\"Average Positive Score\"]\n    negative_score = results[\"Average Negative Score\"]\n    score_difference = positive_score - negative_score\n\n    poet_names.append(alias)\n    score_differences.append(score_difference)\n\n# Sort the poet names and score differences by score differences in descending order\nsorted_poet_names, sorted_score_differences = zip(*sorted(zip(poet_names, score_differences), key=lambda x: x[1], reverse=True))\n\n# Create a bar chart of the score differences with poet names on the x-axis\nplt.figure(figsize=(10, 6))\nplt.bar(sorted_poet_names, sorted_score_differences, color='skyblue')\nplt.xlabel(\"Poet\")\nplt.ylabel('Difference between Positive and Negative Scores')\nplt.title('Whose Poetry Is Most Positive?')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y')\nplt.show()\n```\n\n```{python}\n#| echo: true\n#| output-location: slide\n\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport os\n\n# Initialize the VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Function to calculate aggregate sentiment for a collection of sentences\ndef calculate_aggregate_sentiment(sentences):\n    positive_score = 0\n    negative_score = 0\n    neutral_score = 0\n    total_sentences = 0\n\n    for sentence in sentences:\n        sentiment = sia.polarity_scores(sentence)\n        positive_score += sentiment['pos']\n        negative_score += sentiment['neg']\n        neutral_score += sentiment['neu']\n        total_sentences += 1\n\n    if total_sentences > 0:\n        avg_positive_score = positive_score / total_sentences\n        avg_negative_score = negative_score / total_sentences\n        avg_neutral_score = neutral_score / total_sentences\n\n        if avg_positive_score > avg_negative_score:\n            overall_sentiment = \"Positive\"\n        elif avg_positive_score < avg_negative_score:\n            overall_sentiment = \"Negative\"\n        else:\n            overall_sentiment = \"Neutral\"\n\n        return {\n            \"Total Sentences Analyzed\": total_sentences,\n            \"Average Positive Score\": avg_positive_score,\n            \"Average Negative Score\": avg_negative_score,\n            \"Average Neutral Score\": avg_neutral_score,\n            \"Overall Sentiment\": overall_sentiment,\n        }\n    else:\n        return {\"No sentences with 're-' words found for analysis.\"}\n\n# Specify the directories you want to process with aliases\ncorpus_directories = {\n \n        '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/swinburne/swinburne_noBP': \"AC Swinburne\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/hardy/hardy_noBP': \"Thomas Hardy\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/field/field_NoBP': \"Michael Field\",\n    '/home/adammazel/Documents/Digital_Scholarship/re-victorian-poetry/cta/rossetti_dg/rossetti_dg_NoBP': \"DG Rossetti\",\n       # Add more directories here\n}\n\n# Process each directory\nfor corpus_directory, alias in corpus_directories.items():\n    sentences = []\n\n    # Read and preprocess the text files in the corpus\n    for filename in os.listdir(corpus_directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(corpus_directory, filename), 'r', encoding='utf-8') as file:\n                text = file.read()\n                sentences += nltk.sent_tokenize(text)\n\n    # Filter sentences with words starting with \"re-\"\n    sentences_with_re = [sentence for sentence in sentences if any(word.lower().startswith(\"re-\") for word in nltk.word_tokenize(sentence))]\n\n    # Calculate aggregate sentiment for the filtered sentences\n    results = calculate_aggregate_sentiment(sentences_with_re)\n\n    # Print results for the current directory\n    print(f\"{alias}\")\n    for key, value in results.items():\n        print(f\"{key}: {value}\")\n    print()\n```\n\n## Results: Take Aways\n\n|                          | Poetry: More Positive | Poetry: More Negative |\n|--------------------------| ----------------------| ----------------------|\n| **\"Re-\": More Positive** | Field                 | Rossetti              |\n| **\"Re-\": More Negative** | Swinburne             | Hardy                 |\n\n- Field: Consistently Positive\n- Hardy: Consistently Negative\n- Swinburne: \"Re-\" tends to be in negative contexts / have negative connotations\n    - ressurection / life after death: negative connotations for Swinburne?\n\n## Conclusion\n\n- Take Aways\n    - small details (re-) <--> large data / methods / trends\n    - No conclusions: EDA\n\n- Next Steps\n    - \"re-\" context / patterns in surrounding language\n        - collocates\n    - \"re-\" omissions\n        - Why so rare in DGR?\n    - Re-familiarize myself with poetry\n    - Expand data / scope (other poets)\n\n- **Thank You!**\n- Contact Info\n    - Adam Mazel\n    - Digital Publishing Librarian\n    - Indiana University Bloomington\n    - [amazel@iu.edu](mailto:amazel@iu.edu)","srcMarkdownNoYaml":""},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":true,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","embed-resources":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en-US","fig-responsive":true,"quarto-version":"1.3.450","auto-stretch":true,"title":"Thirteen* Digital Ways of Looking at -re- in Victorian Poetry","date":"11 November 2023","author":"Adam Mazel, Digital Publishing Librarian","institute":"Scholarly Communication Department, IUB Libraries","preloadIframes":true,"output-location":"slide","scrollable":true,"overview":true,"touch":true,"progress":true,"slideNumber":true,"previewLinks":true,"transition":"slide","keywords":["Digital Humanities","Text Mining","Victorian Poetry","re"]}}},"projectFormats":["revealjs"]}